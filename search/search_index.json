{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computational models of learning and decision-making","text":"<p>This repository contains a selection of useful functions for modelling of behaviour in learning and decision-making tasks.</p> <p>All of these functions are written on top of JAX. As a result, this code may not work on Windows unless using WSL.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install the package directly from GitHub using <code>pip</code>:</p> <pre><code>pip install git+https://github.com/the-wise-lab/behavioural-modelling.git\n</code></pre> <p>Alternatively, clone the repository and run <code>pip install . -e</code> from the root directory.</p> <p>For example:</p> <pre><code>git clone https://github.com/the-wise-lab/behavioural-modelling.git\ncd behavioural-modelling\npip install -e . \n</code></pre> <p>This will install an editable version of the package, meaning that you can modify the code and the changes will be reflected in the package.</p> <p>It can then be used as a regular Python package:</p> <pre><code>from behavioural_modelling.decision_rules import softmax\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation can be found here.</p>"},{"location":"#examples","title":"Examples","text":"<p>Example Jupyter notebooks can be found in the <code>examples</code> directory.</p>"},{"location":"#general-principles","title":"General principles","text":"<p>The intention of this package to provide functions implementing specific behavioural models, not for fitting these models. The idea is that these functions are modular and can be used in a variety of contexts, including fitting, simulation, and analysis.</p> <p>A scenario case might involve:</p> <ol> <li>Defining a model.</li> <li>Simulating data from that model for multiple trials and multiple subjects.</li> <li>Performing model fitting based on this.</li> </ol> <p>This package is intended to perform step (1) only.</p>"},{"location":"examples/decisions/decision_rules/","title":"Decision rules","text":"In\u00a0[4]: Copied! <pre>from behavioural_modelling.decision_rules import softmax, softmax_subtract_max, softmax_difference\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> from behavioural_modelling.decision_rules import softmax, softmax_subtract_max, softmax_difference import matplotlib.pyplot as plt import numpy as np In\u00a0[5]: Copied! <pre># Create test values, 5 trials, 3 options with random values ranging from -1 to 1\nrng = np.random.default_rng(12345)\ntest_values = rng.uniform(-1, 1, size=(5, 3))\nprint(test_values)\n</pre> # Create test values, 5 trials, 3 options with random values ranging from -1 to 1 rng = np.random.default_rng(12345) test_values = rng.uniform(-1, 1, size=(5, 3)) print(test_values) <pre>[[-0.54532796 -0.36648332  0.59473091]\n [ 0.35250934 -0.2177809  -0.33437214]\n [ 0.19661751 -0.62653163  0.34551209]\n [ 0.88360573 -0.50350857  0.8977623 ]\n [ 0.33447491 -0.80820413 -0.11632067]]\n</pre> <p>And we can plot these as a heatmap to see the values visually.</p> In\u00a0[8]: Copied! <pre>## Plot as a heatmap\nplt.imshow(test_values, cmap=\"hot\", interpolation=\"nearest\")\nplt.xlabel(\"Option\")\nplt.ylabel(\"Trial\")\nplt.colorbar()\n</pre> ## Plot as a heatmap plt.imshow(test_values, cmap=\"hot\", interpolation=\"nearest\") plt.xlabel(\"Option\") plt.ylabel(\"Trial\") plt.colorbar() Out[8]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f0faa0bc820&gt;</pre> In\u00a0[10]: Copied! <pre># Apply softmax to get choice probabilities\nchoice_probabilities = softmax(test_values)\nprint(choice_probabilities)\n</pre> # Apply softmax to get choice probabilities choice_probabilities = softmax(test_values) print(choice_probabilities) <pre>[[0.1878715  0.22466329 0.5874652 ]\n [0.48344117 0.27331895 0.2432399 ]\n [0.38467494 0.16889033 0.44643474]\n [0.44168591 0.11033098 0.44798315]\n [0.5112254  0.16306217 0.32571244]]\n</pre> In\u00a0[13]: Copied! <pre># Plot as a heatmap\nplt.imshow(choice_probabilities, cmap=\"hot\", interpolation=\"nearest\")\nplt.xlabel(\"Option\")\nplt.ylabel(\"Trial\")\nplt.colorbar()\n</pre> # Plot as a heatmap plt.imshow(choice_probabilities, cmap=\"hot\", interpolation=\"nearest\") plt.xlabel(\"Option\") plt.ylabel(\"Trial\") plt.colorbar() Out[13]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f0fa86dc730&gt;</pre> In\u00a0[31]: Copied! <pre># Generate a range of values from -1 to 1 for a single trial\ntest_values = np.linspace(-2, 2, 100)[:, None]\n\n# Add 1-value as a second option\ntest_values = np.hstack((test_values, (2 - (1 + test_values)) - 1))\n\n# Apply softmax to get choice probabilities\nchoice_probabilities = softmax(test_values)\n\n# Plot function\nplt.plot(test_values[:, 0], choice_probabilities[:, 0])\n</pre> # Generate a range of values from -1 to 1 for a single trial test_values = np.linspace(-2, 2, 100)[:, None]  # Add 1-value as a second option test_values = np.hstack((test_values, (2 - (1 + test_values)) - 1))  # Apply softmax to get choice probabilities choice_probabilities = softmax(test_values)  # Plot function plt.plot(test_values[:, 0], choice_probabilities[:, 0]) Out[31]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f0f8022c550&gt;]</pre> In\u00a0[41]: Copied! <pre># Define temperatures\ntemperatures = [0.1, 0.5, 1, 2, 5, 10]\n\n# Plot softmax functions for each temperature, using viridis colourmap\nfor i, temperature in enumerate(temperatures):\n    plt.plot(\n        test_values[:, 0],\n        softmax(test_values, temperature)[:, 0],\n        label=f\"temp={temperature}\",\n        c=plt.cm.viridis(i / len(temperatures))\n    )\n\n# Add legend\nplt.legend()\n</pre> # Define temperatures temperatures = [0.1, 0.5, 1, 2, 5, 10]  # Plot softmax functions for each temperature, using viridis colourmap for i, temperature in enumerate(temperatures):     plt.plot(         test_values[:, 0],         softmax(test_values, temperature)[:, 0],         label=f\"temp={temperature}\",         c=plt.cm.viridis(i / len(temperatures))     )  # Add legend plt.legend()  Out[41]: <pre>&lt;matplotlib.legend.Legend at 0x7f0f7467e640&gt;</pre> In\u00a0[171]: Copied! <pre># Generate a range of values from -1 to 1 for a single trial\ntest_values = np.linspace(-2, 2, 100)[:, None]\n\n# Add 1-value as a second option\ntest_values = np.hstack((test_values, (2 - (1 + test_values)) - 1))\n\n# Define temperatures\ntemperatures = [0.1, 0.5, 1, 2, 5, 10]\n\n# Plot softmax functions for each temperature, using viridis colourmap\nfor i, temperature in enumerate(temperatures):\n    plt.plot(\n        test_values[:, 0],\n        softmax_subtract_max(test_values, temperature)[:, 0],\n        label=f\"temp={temperature}\",\n        c=plt.cm.viridis(i / len(temperatures))\n    )\n\n# Add legend\nplt.legend()\n</pre> # Generate a range of values from -1 to 1 for a single trial test_values = np.linspace(-2, 2, 100)[:, None]  # Add 1-value as a second option test_values = np.hstack((test_values, (2 - (1 + test_values)) - 1))  # Define temperatures temperatures = [0.1, 0.5, 1, 2, 5, 10]  # Plot softmax functions for each temperature, using viridis colourmap for i, temperature in enumerate(temperatures):     plt.plot(         test_values[:, 0],         softmax_subtract_max(test_values, temperature)[:, 0],         label=f\"temp={temperature}\",         c=plt.cm.viridis(i / len(temperatures))     )  # Add legend plt.legend()  Out[171]: <pre>&lt;matplotlib.legend.Legend at 0x7f0efc5b3100&gt;</pre> <p>Critically, this means it gives non-NaN results even when the values are large (although the results might look a bit strange).</p> In\u00a0[174]: Copied! <pre># Generate a range of values from -1 to 1 for a single trial\ntest_values = np.linspace(-9999, 9999, 100)[:, None]\n\n# Add 1-value as a second option\ntest_values = np.hstack((test_values, (2 - (1 + test_values)) - 1))\n\n# Define temperatures\ntemperatures = [0.1, 0.5, 1, 2, 5, 10]\n\n# Plot softmax functions for each temperature, using viridis colourmap\nfor i, temperature in enumerate(temperatures):\n    plt.plot(\n        test_values[:, 0],\n        softmax_subtract_max(test_values, temperature)[:, 0],\n        label=f\"temp={temperature}\",\n        c=plt.cm.viridis(i / len(temperatures))\n    )\n\n# Add legend\nplt.legend()\n</pre> # Generate a range of values from -1 to 1 for a single trial test_values = np.linspace(-9999, 9999, 100)[:, None]  # Add 1-value as a second option test_values = np.hstack((test_values, (2 - (1 + test_values)) - 1))  # Define temperatures temperatures = [0.1, 0.5, 1, 2, 5, 10]  # Plot softmax functions for each temperature, using viridis colourmap for i, temperature in enumerate(temperatures):     plt.plot(         test_values[:, 0],         softmax_subtract_max(test_values, temperature)[:, 0],         label=f\"temp={temperature}\",         c=plt.cm.viridis(i / len(temperatures))     )  # Add legend plt.legend()  Out[174]: <pre>&lt;matplotlib.legend.Legend at 0x7f0ebebd8100&gt;</pre>"},{"location":"examples/decisions/decision_rules/#decision-rules","title":"Decision rules\u00b6","text":""},{"location":"examples/decisions/decision_rules/#softmax-function","title":"Softmax function\u00b6","text":""},{"location":"examples/decisions/decision_rules/#create-test-values","title":"Create test values\u00b6","text":"<p>The decision rule functions (e.g., <code>softmax()</code>) expect a 2D array of values, where the 1st dimension corresponds to the trial and the second dimension corresponds to the options.</p> <p>So, for example if we had 10 trials and 3 options, we would have a 2D array with shape <code>(10, 3)</code>.</p> <p>Here we create some random values in the range <code>[-1, 1]</code> to use as our test values.</p>"},{"location":"examples/decisions/decision_rules/#use-the-softmax-function","title":"Use the softmax function\u00b6","text":"<p>We then apply the softmax function to transform these values into action probabilities.</p>"},{"location":"examples/decisions/decision_rules/#plot-the-relationship-between-value-and-probability","title":"Plot the relationship between value and probability\u00b6","text":"<p>To demonstrate the relationship between value and probability, we can plot the softmax function for a range of values.</p>"},{"location":"examples/decisions/decision_rules/#demonstrate-the-effect-of-temperature","title":"Demonstrate the effect of temperature\u00b6","text":"<p>We can also demonstrate the effect of temperature on the softmax function. The temperature parameter controls the steepness of the softmax function. Higher temperatures result in a flatter softmax function, which means that the action probabilities are closer to each other.</p>"},{"location":"examples/decisions/decision_rules/#softmax-function-subtracting-the-maximum-value","title":"Softmax function, subtracting the maximum value\u00b6","text":"<p>In some applications we might have large values, which can cause numerical issues when computing the softmax function. To avoid this, we can subtract the maximum value from all values before computing the softmax function. This does not change the result, but can avoid these issues.</p>"},{"location":"examples/decisions/simulating_choices/","title":"Simulating choices","text":"In\u00a0[1]: Copied! <pre>from behavioural_modelling.utils import choice_from_action_p\nimport jax\nimport numpy as np\nimport jax.numpy as jnp\n</pre> from behavioural_modelling.utils import choice_from_action_p import jax import numpy as np import jax.numpy as jnp In\u00a0[2]: Copied! <pre>key = jax.random.PRNGKey(0)\n</pre> key = jax.random.PRNGKey(0) <pre>WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <p>We can then generate some action probabilities.</p> In\u00a0[3]: Copied! <pre>rng = np.random.default_rng(12345)\n\n# Generate random probabilities\nprobs = rng.uniform(size=(40, 100, 3))\n</pre> rng = np.random.default_rng(12345)  # Generate random probabilities probs = rng.uniform(size=(40, 100, 3)) <p>And generate choices.</p> In\u00a0[8]: Copied! <pre>choices = choice_from_action_p(key, probs)\nprint(choices)\n</pre> choices = choice_from_action_p(key, probs) print(choices) <pre>[[2 1 2 ... 2 1 2]\n [1 0 1 ... 1 1 1]\n [1 2 1 ... 1 1 2]\n ...\n [2 1 1 ... 1 1 1]\n [0 1 1 ... 1 2 1]\n [1 2 1 ... 1 0 2]]\n</pre>"},{"location":"examples/decisions/simulating_choices/#simulating-choices","title":"Simulating choices\u00b6","text":"<p>Often we will want to create simulated data containing choices made based on the values learned by a particular model.</p>"},{"location":"examples/decisions/simulating_choices/#example-of-generating-choices-based-on-choice-probabilities","title":"Example of generating choices based on choice probabilities\u00b6","text":"<p>We can use the <code>choice_from_action_p</code> function to generate choices based on the action probabilities output by a model.</p> <p>As this is built using JAX, it requires a random key to be passed in. This can be generated using the <code>jax.random.PRNGKey</code> function.</p>"},{"location":"examples/learning/beta_learning_models/","title":"Beta Model Examples","text":"In\u00a0[271]: Copied! <pre>from behavioural_modelling.learning.beta_models import beta_mean_var, multiply_beta_by_scalar, average_betas, sum_betas, leaky_beta_update\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import beta\nfrom functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom numpy.typing import ArrayLike\n</pre> from behavioural_modelling.learning.beta_models import beta_mean_var, multiply_beta_by_scalar, average_betas, sum_betas, leaky_beta_update import matplotlib.pyplot as plt import numpy as np import seaborn as sns from scipy.stats import beta from functools import partial import jax import jax.numpy as jnp from numpy.typing import ArrayLike In\u00a0[3]: Copied! <pre>a = 1\nb = 1\n\nmean, var = beta_mean_var(np.array([[a, b]]))\n\nprint(f\"Mean: {mean}\")\nprint(f\"Variance: {var}\")\n</pre> a = 1 b = 1  mean, var = beta_mean_var(np.array([[a, b]]))  print(f\"Mean: {mean}\") print(f\"Variance: {var}\") <pre>WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <pre>Mean: [0.5]\nVariance: [0.08333334]\n</pre> <p>This can be applied to multiple sets of shape parameters at once by passing an array of shape <code>(n, 2)</code> to the <code>beta_mean_var()</code> function.</p> In\u00a0[4]: Copied! <pre>N_OPTIONS = 10\n\n# Create an array of beta params\nrng = np.random.default_rng(0)\nbeta_params = rng.uniform(1, 10, size=(N_OPTIONS, 2))\n\n# Calculate mean and variance of each beta distribution\nmean, var = beta_mean_var(beta_params)\n\nprint(mean)\nprint(var)\n</pre> N_OPTIONS = 10  # Create an array of beta params rng = np.random.default_rng(0) beta_params = rng.uniform(1, 10, size=(N_OPTIONS, 2))  # Calculate mean and variance of each beta distribution mean, var = beta_mean_var(beta_params)  print(mean) print(var) <pre>[0.66261494 0.5436965  0.474468   0.46057996 0.38493064 0.89061487\n 0.8700188  0.74566895 0.5988767  0.43490762]\n[0.02003061 0.07053018 0.01345338 0.0165353  0.01451772 0.00939683\n 0.01026292 0.01701204 0.01535782 0.02586546]\n</pre> In\u00a0[5]: Copied! <pre>a, b = (5, 1)\nc = 0.5\n</pre> a, b = (5, 1) c = 0.5 <p>We can then run simulations where we multiply draws from the beta distribution by the scalar and plot the resulting distribution.</p> In\u00a0[6]: Copied! <pre>N_SIMS = 100000\n\noriginal_ps = np.zeros(N_SIMS)\nscaled_ps = np.zeros(N_SIMS)\n\nfor i in range(N_SIMS):\n    original_ps[i] = np.random.beta(a, b)\n    scaled_ps[i] = original_ps[i] * c\n\nsns.kdeplot(original_ps, label='Original')\nsns.kdeplot(scaled_ps, label='Scaled')\nplt.xlim(0, 1)\nplt.xlabel(\"p\")\n\nplt.legend()\n\nprint('Mean = {0}, Variance = {1}'.format(np.mean(scaled_ps), np.var(scaled_ps)))\n</pre> N_SIMS = 100000  original_ps = np.zeros(N_SIMS) scaled_ps = np.zeros(N_SIMS)  for i in range(N_SIMS):     original_ps[i] = np.random.beta(a, b)     scaled_ps[i] = original_ps[i] * c  sns.kdeplot(original_ps, label='Original') sns.kdeplot(scaled_ps, label='Scaled') plt.xlim(0, 1) plt.xlabel(\"p\")  plt.legend()  print('Mean = {0}, Variance = {1}'.format(np.mean(scaled_ps), np.var(scaled_ps))) <pre>Mean = 0.4167975690880757, Variance = 0.0049450535578850485\n</pre> <p>If we use the <code>multiply_beta_by_scalar()</code> function to approximate the result of multiplying the beta distribution by the scalar, we can see that the mean and variance of the resulting distribution are the near enough the same as the true distribution, but the skew and kurtosis are slightly different.</p> In\u00a0[7]: Copied! <pre>x = np.linspace(0, 1, 100)\nplt.plot(x, beta.pdf(x, a, b), label='Original')\n\n# Multiply beta distribution by scalar\nscalar = 0.5\na_new, b_new = multiply_beta_by_scalar(np.array([[a, b]]), scalar)[0]\n# plot\nplt.plot(x, beta.pdf(x, a_new, b_new), label='Scaled')\n\nplt.legend()\n\n# Labels\nplt.xlabel(\"p\")\nplt.ylabel(\"Density\")\n\n# get mean etc of new beta distribution using scipy beta\nmean, var = beta.stats(a_new, b_new, moments=\"mv\")\nprint('Mean = {0}, Variance = {1}'.format(mean, var))\n</pre> x = np.linspace(0, 1, 100) plt.plot(x, beta.pdf(x, a, b), label='Original')  # Multiply beta distribution by scalar scalar = 0.5 a_new, b_new = multiply_beta_by_scalar(np.array([[a, b]]), scalar)[0] # plot plt.plot(x, beta.pdf(x, a_new, b_new), label='Scaled')  plt.legend()  # Labels plt.xlabel(\"p\") plt.ylabel(\"Density\")  # get mean etc of new beta distribution using scipy beta mean, var = beta.stats(a_new, b_new, moments=\"mv\") print('Mean = {0}, Variance = {1}'.format(mean, var))  <pre>Mean = 0.4166666567325592, Variance = 0.004960318095982075\n</pre> In\u00a0[8]: Copied! <pre>a1, b1 = (1, 6)\na2, b2 = (2, 6)\n\n# plot\nplt.plot(x, beta.pdf(x, a1, b1), label='1')\nplt.plot(x, beta.pdf(x, a2, b2), label='2')\n\n# Get average\nsum_dist = sum_betas(np.array([[a1, b1]]), np.array([[a2, b2]]))\n\n# plot\nplt.plot(x, beta.pdf(x, sum_dist[0, 0], sum_dist[0, 1]), label='Sum')\n\nplt.legend()\n</pre> a1, b1 = (1, 6) a2, b2 = (2, 6)  # plot plt.plot(x, beta.pdf(x, a1, b1), label='1') plt.plot(x, beta.pdf(x, a2, b2), label='2')  # Get average sum_dist = sum_betas(np.array([[a1, b1]]), np.array([[a2, b2]]))  # plot plt.plot(x, beta.pdf(x, sum_dist[0, 0], sum_dist[0, 1]), label='Sum')  plt.legend() Out[8]: <pre>&lt;matplotlib.legend.Legend at 0x7f26f0741670&gt;</pre> In\u00a0[38]: Copied! <pre>a1, b1 = (5, 1)\na2, b2 = (1, 5)\n\n# plot\nplt.plot(x, beta.pdf(x, a1, b1), label='1')\nplt.plot(x, beta.pdf(x, a2, b2), label='2')\n\n# Get average\navg = average_betas(np.array([[a1, b1]]), np.array([[a2, b2]]))\n\n# plot\nplt.plot(x, beta.pdf(x, avg[0, 0], avg[0, 1]), label='Average')\n\nplt.legend()\n</pre> a1, b1 = (5, 1) a2, b2 = (1, 5)  # plot plt.plot(x, beta.pdf(x, a1, b1), label='1') plt.plot(x, beta.pdf(x, a2, b2), label='2')  # Get average avg = average_betas(np.array([[a1, b1]]), np.array([[a2, b2]]))  # plot plt.plot(x, beta.pdf(x, avg[0, 0], avg[0, 1]), label='Average')  plt.legend() Out[38]: <pre>&lt;matplotlib.legend.Legend at 0x7f27080372e0&gt;</pre> <p>If weights are used, the average is a weighted average:</p> In\u00a0[22]: Copied! <pre>a1, b1 = (5, 1)\na2, b2 = (1, 5)\n\n# plot\nplt.plot(x, beta.pdf(x, a1, b1), label='1')\nplt.plot(x, beta.pdf(x, a2, b2), label='2')\n\n# Get average\navg = average_betas(np.array([[a1, b1]]), np.array([[a2, b2]]), W1=0.1, W2=0.9)\n\n# plot\nplt.plot(x, beta.pdf(x, avg[0, 0], avg[0, 1]), label='Average')\n\nplt.legend()\n</pre> a1, b1 = (5, 1) a2, b2 = (1, 5)  # plot plt.plot(x, beta.pdf(x, a1, b1), label='1') plt.plot(x, beta.pdf(x, a2, b2), label='2')  # Get average avg = average_betas(np.array([[a1, b1]]), np.array([[a2, b2]]), W1=0.1, W2=0.9)  # plot plt.plot(x, beta.pdf(x, avg[0, 0], avg[0, 1]), label='Average')  plt.legend() Out[22]: <pre>&lt;matplotlib.legend.Legend at 0x7f864072b5b0&gt;</pre> In\u00a0[174]: Copied! <pre># Create three beta distributions, initialised at (1, 1), of shape (3, 2)\nbeta_params = np.ones((3, 2))\nprint(beta_params)\n</pre> # Create three beta distributions, initialised at (1, 1), of shape (3, 2) beta_params = np.ones((3, 2)) print(beta_params) <pre>[[1. 1.]\n [1. 1.]\n [1. 1.]]\n</pre> <p>On a given trial, the subject selects the 2nd option. We represent the choice as a one-hot encoded array of shape <code>(N_OPTIONS,)</code>.</p> In\u00a0[173]: Copied! <pre>choice = np.zeros(3)\nchoice[1] = 1\nprint(choice)\n</pre> choice = np.zeros(3) choice[1] = 1 print(choice) <pre>[0. 1. 0.]\n</pre> <p>The outcome on this trial is 1 (reward). The function assumes that a single outcome is received on each trial (i.e., we're not seeing different outcomes for each option). As we're modelling the probability of a given outcome, the outcome must be binary (0 or 1).</p> In\u00a0[175]: Copied! <pre>outcome = 1\n</pre> outcome = 1 <p>We need to also choose some parameters for the model. We'll use $\\lambda = 0.9$, $\\tau^{+} = 0.5$, and $\\tau^{-} = 0.1$. This means that there will be little decay, and that positive outcomes will result in a large increase in the probability of reward, whereas negative outcomes will result in a small decrease in the probability of reward.</p> In\u00a0[182]: Copied! <pre>lambda_ = 0.9  # can't use plain lambda as it's a reserved keyword in python\ntau_p = 0.5\ntau_n = 0.1\n</pre> lambda_ = 0.9  # can't use plain lambda as it's a reserved keyword in python tau_p = 0.5 tau_n = 0.1 <p>We can now use the update function to update our beta distributions.</p> In\u00a0[179]: Copied! <pre>leaky_beta_update(\n    beta_params,\n    choice,\n    outcome,\n    tau_p,\n    tau_n,\n    lambda_,\n)\n</pre> leaky_beta_update(     beta_params,     choice,     outcome,     tau_p,     tau_n,     lambda_, ) Out[179]: <pre>DeviceArray([[1. , 1. ],\n             [1.5, 1. ],\n             [1. , 1. ]], dtype=float32)</pre> <p>Note that no decay has been applied, as the function does not allow the shape parameters to go below 1. While beta distributions can have shape parameters below 1, these values can produce strange distributions (e.g., with joint peaks at 0 and 1) so for simplicity we don't allow this. If we set our starting shape parameters to a higher value, we can see that the decay is applied.</p> In\u00a0[183]: Copied! <pre>beta_params = np.ones((3, 2)) * 2\n\nleaky_beta_update(\n    beta_params,\n    choice,\n    outcome,\n    tau_p,\n    tau_n,\n    lambda_,\n)\n</pre> beta_params = np.ones((3, 2)) * 2  leaky_beta_update(     beta_params,     choice,     outcome,     tau_p,     tau_n,     lambda_, ) Out[183]: <pre>DeviceArray([[1.9, 1.9],\n             [2.4, 1.9],\n             [1.9, 1.9]], dtype=float32)</pre> <p>If we loop over a set of trials we can see how the probability of reward for each option changes over time.</p> <p>Here, we'll simulate 200 trials where the subject chooses randomly, and the option are rewarded at 0%, 50%, and 100% respectively.</p> In\u00a0[285]: Copied! <pre>N_TRIALS = 200\n\nrng = np.random.default_rng(0)\n\n# Generate outcomes for every trial\noutcomes = np.zeros((N_TRIALS, 3))\noutcomes[rng.choice(N_TRIALS, size=50, replace=False), 1] = 1\noutcomes[:, 2] = 1\n\n# Generate choices for every trial\nchoices = np.zeros((N_TRIALS, 3))\nchoices[range(N_TRIALS), rng.integers(0, 3, N_TRIALS)] = 1\n\n# Starting beta params\nbeta_params = np.ones((N_TRIALS, 3, 2))\n\nfor i in range(N_TRIALS - 1):\n\n    beta_params[i + 1, ...] = leaky_beta_update(\n        beta_params[i, ...],\n        choices[i, ...],\n        outcomes[i, ...],\n        tau_p,\n        tau_n,\n        lambda_,\n    )\n</pre> N_TRIALS = 200  rng = np.random.default_rng(0)  # Generate outcomes for every trial outcomes = np.zeros((N_TRIALS, 3)) outcomes[rng.choice(N_TRIALS, size=50, replace=False), 1] = 1 outcomes[:, 2] = 1  # Generate choices for every trial choices = np.zeros((N_TRIALS, 3)) choices[range(N_TRIALS), rng.integers(0, 3, N_TRIALS)] = 1  # Starting beta params beta_params = np.ones((N_TRIALS, 3, 2))  for i in range(N_TRIALS - 1):      beta_params[i + 1, ...] = leaky_beta_update(         beta_params[i, ...],         choices[i, ...],         outcomes[i, ...],         tau_p,         tau_n,         lambda_,     ) <p>We can then get the mean and variance of these distributions to show how the probability of reward changes over time.</p> In\u00a0[273]: Copied! <pre>mean, var = beta_mean_var(beta_params)\n\nplt.figure(figsize=(10, 3))\n\n# Plot mean and variance of each option as line with shaded region\nplt.plot(mean[:, 0], label='Option 1')\nplt.fill_between(np.arange(N_TRIALS), mean[:, 0] - var[:, 0], mean[:, 0] + var[:, 0], alpha=0.2)\n\nplt.plot(mean[:, 1], label='Option 2')\nplt.fill_between(np.arange(N_TRIALS), mean[:, 1] - var[:, 1], mean[:, 1] + var[:, 1], alpha=0.2)\n\nplt.plot(mean[:, 2], label='Option 3')\nplt.fill_between(np.arange(N_TRIALS), mean[:, 2] - var[:, 2], mean[:, 2] + var[:, 2], alpha=0.2)\n\nplt.xlabel(\"Trial\")\nplt.ylabel(\"p\")\n\nplt.legend()\n</pre> mean, var = beta_mean_var(beta_params)  plt.figure(figsize=(10, 3))  # Plot mean and variance of each option as line with shaded region plt.plot(mean[:, 0], label='Option 1') plt.fill_between(np.arange(N_TRIALS), mean[:, 0] - var[:, 0], mean[:, 0] + var[:, 0], alpha=0.2)  plt.plot(mean[:, 1], label='Option 2') plt.fill_between(np.arange(N_TRIALS), mean[:, 1] - var[:, 1], mean[:, 1] + var[:, 1], alpha=0.2)  plt.plot(mean[:, 2], label='Option 3') plt.fill_between(np.arange(N_TRIALS), mean[:, 2] - var[:, 2], mean[:, 2] + var[:, 2], alpha=0.2)  plt.xlabel(\"Trial\") plt.ylabel(\"p\")  plt.legend() Out[273]: <pre>&lt;matplotlib.legend.Legend at 0x7f2620446cd0&gt;</pre> <p>All of these functions are designed to work with JAX as it can speed things up. Most importanly, we can use it with <code>vmap</code> to vectorise the function across subjects/task blocks, making this much faster (and possible to run on GPUs).This also means that they can be used within NumPyro for MCMC sampling.</p> <p>Rather than running the update function using a Python loop, we can use <code>jax.lax.scan</code> to create a compiled function that runs the update function over a set of trials.</p> <p>First, we need to define a wrapper function for the update function. This is because <code>jax.lax.scan</code> requires a function that takes only two arguments (one that is updated on each trial, and one that provides values to be used by the function on each trial), so we need to wrap the update function to take a tuple of arguments representing the outcomes and choices.</p> In\u00a0[264]: Copied! <pre>def leaky_beta_update_wrapper(\n    beta_params: ArrayLike,\n    choice_outcomes: ArrayLike,\n    tau_p: float,\n    tau_n: float,\n    decay: float\n) -&gt; jnp.ndarray:\n\n    # Unpack choices and _outcomes\n    choice, outcome = choice_outcomes\n\n    # Update\n    beta_params_updated = leaky_beta_update(\n        beta_params,\n        choice,\n        outcome,\n        tau_p,\n        tau_n,\n        decay,\n    )\n\n    # Return updated params, both for the next iteration\n    # and to keep track of values on each iteration\n    return beta_params_updated, beta_params_updated\n</pre> def leaky_beta_update_wrapper(     beta_params: ArrayLike,     choice_outcomes: ArrayLike,     tau_p: float,     tau_n: float,     decay: float ) -&gt; jnp.ndarray:      # Unpack choices and _outcomes     choice, outcome = choice_outcomes      # Update     beta_params_updated = leaky_beta_update(         beta_params,         choice,         outcome,         tau_p,         tau_n,         decay,     )      # Return updated params, both for the next iteration     # and to keep track of values on each iteration     return beta_params_updated, beta_params_updated <p>Because the parameters are fixed across trials, we can bake them into the function using <code>functools.partial</code>.</p> In\u00a0[274]: Copied! <pre># Use partial to store the fixed parameters\nleaky_beta_update_partial = partial(\n    leaky_beta_update_wrapper,\n    tau_p=tau_p,\n    tau_n=tau_n,\n    decay=lambda_,\n)\n</pre> # Use partial to store the fixed parameters leaky_beta_update_partial = partial(     leaky_beta_update_wrapper,     tau_p=tau_p,     tau_n=tau_n,     decay=lambda_, ) <p>We can then run the function using <code>jax.lax.scan</code> to update the beta distributions over a set of trials.</p> In\u00a0[275]: Copied! <pre># Starting beta params\nbeta_params = np.ones((3, 2))\n\n# Run the update\n_, beta_params_updated = jax.lax.scan(\n    leaky_beta_update_partial,\n    beta_params,\n    (choices, outcomes)\n)\n\nmean, var = beta_mean_var(beta_params_updated)\n\nplt.figure(figsize=(10, 3))\n\n# Plot mean and variance of each option as line with shaded region\nplt.plot(mean[:, 0], label='Option 1')\nplt.fill_between(np.arange(N_TRIALS), mean[:, 0] - var[:, 0], mean[:, 0] + var[:, 0], alpha=0.2)\n\nplt.plot(mean[:, 1], label='Option 2')\nplt.fill_between(np.arange(N_TRIALS), mean[:, 1] - var[:, 1], mean[:, 1] + var[:, 1], alpha=0.2)\n\nplt.plot(mean[:, 2], label='Option 3')\nplt.fill_between(np.arange(N_TRIALS), mean[:, 2] - var[:, 2], mean[:, 2] + var[:, 2], alpha=0.2)\n\nplt.xlabel(\"Trial\")\nplt.ylabel(\"p\")\n\nplt.legend()\n</pre> # Starting beta params beta_params = np.ones((3, 2))  # Run the update _, beta_params_updated = jax.lax.scan(     leaky_beta_update_partial,     beta_params,     (choices, outcomes) )  mean, var = beta_mean_var(beta_params_updated)  plt.figure(figsize=(10, 3))  # Plot mean and variance of each option as line with shaded region plt.plot(mean[:, 0], label='Option 1') plt.fill_between(np.arange(N_TRIALS), mean[:, 0] - var[:, 0], mean[:, 0] + var[:, 0], alpha=0.2)  plt.plot(mean[:, 1], label='Option 2') plt.fill_between(np.arange(N_TRIALS), mean[:, 1] - var[:, 1], mean[:, 1] + var[:, 1], alpha=0.2)  plt.plot(mean[:, 2], label='Option 3') plt.fill_between(np.arange(N_TRIALS), mean[:, 2] - var[:, 2], mean[:, 2] + var[:, 2], alpha=0.2)  plt.xlabel(\"Trial\") plt.ylabel(\"p\")  plt.legend() Out[275]: <pre>&lt;matplotlib.legend.Legend at 0x7f2620093eb0&gt;</pre> <p>We can also use JAX's <code>vmap</code> function to vectorise the function across subjects, allowing us to run the function on multiple subjects efficiently without needing to explicitly defined the function in a vectorised manner.</p> In\u00a0[352]: Copied! <pre>def leaky_beta_update_vmap_function(\n    beta_params: ArrayLike,\n    choices: ArrayLike,\n    outcomes: ArrayLike,\n    tau_p: float, \n    tau_n: float, \n    decay: float\n):\n    \n    # Pack choices and outcomes into a tuple\n    choice_outcomes = (choices, outcomes)\n\n    # Use partial to store the fixed parameters\n    leaky_beta_update_partial = partial(\n        leaky_beta_update_wrapper,\n        tau_p=tau_p,\n        tau_n=tau_n,\n        decay=decay,\n    )\n\n    # Run the update\n    _, beta_params_updated = jax.lax.scan(\n        leaky_beta_update_partial,\n        beta_params,\n        (choices, outcomes)\n    )\n\n    return beta_params_updated\n\n# Model parameter values for each subject\nN_SUBJECTS = 50\ntau_p = rng.uniform(0.1, 1, N_SUBJECTS)\ntau_n = rng.uniform(0.1, 1, N_SUBJECTS)\nlambda_ = rng.uniform(0.1, 1, N_SUBJECTS)\n\n# Use vmap to vectorise over subjects\nbeta_params_vmap = jax.vmap(\n    leaky_beta_update_vmap_function,\n    in_axes=(None, None, None, 0, 0, 0)\n)\n\nbeta_params_updated = beta_params_vmap(\n    beta_params,\n    choices,\n    outcomes,\n    tau_p,\n    tau_n,\n    lambda_,\n)\n\nprint(beta_params_updated.shape)\n</pre> def leaky_beta_update_vmap_function(     beta_params: ArrayLike,     choices: ArrayLike,     outcomes: ArrayLike,     tau_p: float,      tau_n: float,      decay: float ):          # Pack choices and outcomes into a tuple     choice_outcomes = (choices, outcomes)      # Use partial to store the fixed parameters     leaky_beta_update_partial = partial(         leaky_beta_update_wrapper,         tau_p=tau_p,         tau_n=tau_n,         decay=decay,     )      # Run the update     _, beta_params_updated = jax.lax.scan(         leaky_beta_update_partial,         beta_params,         (choices, outcomes)     )      return beta_params_updated  # Model parameter values for each subject N_SUBJECTS = 50 tau_p = rng.uniform(0.1, 1, N_SUBJECTS) tau_n = rng.uniform(0.1, 1, N_SUBJECTS) lambda_ = rng.uniform(0.1, 1, N_SUBJECTS)  # Use vmap to vectorise over subjects beta_params_vmap = jax.vmap(     leaky_beta_update_vmap_function,     in_axes=(None, None, None, 0, 0, 0) )  beta_params_updated = beta_params_vmap(     beta_params,     choices,     outcomes,     tau_p,     tau_n,     lambda_, )  print(beta_params_updated.shape)  <pre>(50, 200, 3, 2)\n</pre>"},{"location":"examples/learning/beta_learning_models/#beta-learning-models","title":"Beta learning models\u00b6","text":"<p>Beta models are useful because they can be used to model a wide range of learning tasks where an outcome is binary and probabilistic (e.g., reward/no reward). They represent the probability of the outcome as a beta distribution, updating this distribution based on observed outcomes.</p> <p>This modelling approach is used in the following papers: 1, 2</p>"},{"location":"examples/learning/beta_learning_models/#basic-beta-distribution-functions","title":"Basic beta distribution functions\u00b6","text":"<p>This package provides a number of functions for working with beta distributions.</p>"},{"location":"examples/learning/beta_learning_models/#get-the-mean-and-variance-of-a-beta-distribution","title":"Get the mean and variance of a beta distribution\u00b6","text":"<p>The mean and variance of a beta distribution is calculated as follows:</p> <p>$$\\mu = \\frac{\\alpha}{\\alpha + \\beta}$$</p> <p>$$\\sigma^2 = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}$$</p> <p>where $\\alpha$ and $\\beta$ are the shape parameters of the beta distribution.</p> <p>The <code>beta_mean_var()</code> function calculates the mean and variance of a beta distribution given the shape parameters $\\alpha$ and $\\beta$.</p>"},{"location":"examples/learning/beta_learning_models/#multiplying-a-beta-distribution-by-a-scalar","title":"Multiplying a beta distribution by a scalar\u00b6","text":"<p>Multiplying a beta distribution by a scalar $c$ results in a truncated beta distribution with shape parameters $\\alpha$ and $c \\beta$ and truncation parameter $z$. This is intuitive if we consider that the upper limit on any value drawn from the beta distribution multiplied by $c$ is $c$.</p> <p>However, it can sometimes be useful to be able to approximate this as a beta distribution with the original shape parameters $\\alpha$ and $\\beta$ without truncation (for example, if we later want to sum two beta distributions together, it's not straightforward to sum two truncated distributions when they have different $z$ values).</p> <p>The <code>multiply_beta_by_scalar()</code> function approximates the result of multiplying a beta distribution by a scalar $c$ as a beta distribution with the original shape parameters $\\alpha$ and $\\beta$ without truncation. This approximation results in a beta distribution with the correct mean and variance, but the skew/kurtosis of the distribution will be slightly different to the true distribution.</p> <p>We can demonstrate this empirically via simulation. First, we'll generate a beta distribution with shape parameters $\\alpha = 5$ and $\\beta = 5$, which we'll multiply by scalar $c = 0.5$.</p>"},{"location":"examples/learning/beta_learning_models/#summing-beta-distributions","title":"Summing beta distributions\u00b6","text":"<p>We can sum two beta distributions together - essentially accumulating evidence about the probability of an event from two sources, each of which is represented by a beta distribution. This is useful for modelling learning tasks where we want to model the probability of an event (e.g., reward) based on multiple sources of evidence.</p> <p>Note: This only works if the means of the two beta distributions sum to &lt;=1.</p>"},{"location":"examples/learning/beta_learning_models/#averaging-beta-distributions","title":"Averaging beta distributions\u00b6","text":"<p>We can also average together multiple beta distributions by simply taking the mean of the shape parameters of the beta distributions. We can also weight the beta distributions by scalar weights $w_1$ and $w_2$ to give more weight to one beta distribution than the other:</p> <p>$$\\alpha_{avg} = w_1 \\alpha_1 + w_2 \\alpha_2$$ $$\\beta_{avg} = w_1 \\beta_1 + w_2 \\beta_2$$</p> <p>We allow for separate weights rather than a single weight and its complement as this approach is used in some modelling formulations that combine multiple estimates (e.g., the MF/MB model in Otto et al. (2013)).</p> <p>This is implemented in the <code>average_betas()</code> function. If weights aren't specified, the beta distributions are weighted equally.</p>"},{"location":"examples/learning/beta_learning_models/#asymmetric-leaky-beta-learning-model","title":"Asymmetric leaky beta learning model\u00b6","text":"<p>This model is described in (1, 2) and represents the probability of an outcome as a beta distribution which is updated on each trial.</p> <p>Estimates are updated on each trial according to the following equations:</p> <p>\\begin{gathered} A_i^{t+1}=\\lambda \\cdot A_i^{t}+outcome_t \\cdot \\tau^{+}  \\\\ B_i^{t+1}=\\lambda \\cdot B_i^{t}+(1-outcome_t) \\cdot \\tau^{-} \\end{gathered}</p> <p>Where $\\lambda$ is a decay parameter (lower values result in greater decay), $outcome_t$ is the outcome on trial $t$ (0 or 1), $\\tau^{+}$ is the increment rate for positive outcomes, and $\\tau^{-}$ is the increment rate for negative outcomes. Here $A_i$ and $B_i$ represent the shape parameters of the beta distribution for the $i$ th stimulus (note that these shape parameters are often referred to as $\\alpha$ and $\\beta$, but this terminology gets confusing as it means we have to describe e.g., the $\\beta$ parameter of the $\\beta$ distribution).</p> <p>The update is implemented in the <code>leaky_beta_update()</code> function, which takes a set of beta distributions and updates them according to the above equations.</p> <p>As an example, we can create three beta distributions representing the probability of reward for three options, and update them according to the above equations. We specify the distributions as an array of shape <code>(N_OPTIONS, 2)</code>, and as we have no prior knowledge we can initialise this as an array of ones.</p>"},{"location":"examples/planning/value_iteration/","title":"Dynamic Programming Examples","text":"In\u00a0[1]: Copied! <pre># autorelaod\n%load_ext autoreload\n%autoreload 2\n</pre> # autorelaod %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport jax.numpy as jnp\nfrom behavioural_modelling.planning.dynamic_programming import (\n    solve_value_iteration,\n)\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import jax.numpy as jnp from behavioural_modelling.planning.dynamic_programming import (     solve_value_iteration, ) import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># Simple Gridworld setup\ndef create_gridworld(\n    grid_size: int = 15,\n    discount: float = 0.9,\n    n_rewards: int = 1,\n    seed: int = 42,\n):\n\n    # Initialise RNG\n    rng = np.random.RandomState(seed)\n\n    # Number of states and actions\n    n_states = grid_size**2\n    n_actions = 5  # up, down, left, right, stay\n\n    # Reward function\n    reward_function = np.zeros(n_states)\n    for _ in range(n_rewards):\n        reward_function[rng.randint(n_states)] = 1.0\n\n    # Features matrix: Identity (each state is a feature)\n    features = np.eye(n_states)\n\n    # SAS matrix (state, action, state transition probabilities)\n    sas = np.zeros((n_states, n_actions, n_states))\n\n    # Create transition probabilities for a simple gridworld\n    def state_to_coords(s, size):\n        return s // size, s % size\n\n    def coords_to_state(x, y, size):\n        return x * size + y\n\n    # Get the state transition probabilities\n    for s in range(n_states):\n        x, y = state_to_coords(s, grid_size)\n\n        # Up\n        if x &gt; 0:\n            sas[s, 0, coords_to_state(x - 1, y, grid_size)] = 1.0\n        else:\n            sas[s, 0, s] = 1.0  # Bump into wall\n\n        # Down\n        if x &lt; grid_size - 1:\n            sas[s, 1, coords_to_state(x + 1, y, grid_size)] = 1.0\n        else:\n            sas[s, 1, s] = 1.0  # Bump into wall\n\n        # Left\n        if y &gt; 0:\n            sas[s, 2, coords_to_state(x, y - 1, grid_size)] = 1.0\n        else:\n            sas[s, 2, s] = 1.0  # Bump into wall\n\n        # Right\n        if y &lt; grid_size - 1:\n            sas[s, 3, coords_to_state(x, y + 1, grid_size)] = 1.0\n        else:\n            sas[s, 3, s] = 1.0  # Bump into wall\n\n        # Stay\n        sas[s, 4, s] = 1.0\n\n    return reward_function, features, sas, discount, n_states, n_actions\n</pre> # Simple Gridworld setup def create_gridworld(     grid_size: int = 15,     discount: float = 0.9,     n_rewards: int = 1,     seed: int = 42, ):      # Initialise RNG     rng = np.random.RandomState(seed)      # Number of states and actions     n_states = grid_size**2     n_actions = 5  # up, down, left, right, stay      # Reward function     reward_function = np.zeros(n_states)     for _ in range(n_rewards):         reward_function[rng.randint(n_states)] = 1.0      # Features matrix: Identity (each state is a feature)     features = np.eye(n_states)      # SAS matrix (state, action, state transition probabilities)     sas = np.zeros((n_states, n_actions, n_states))      # Create transition probabilities for a simple gridworld     def state_to_coords(s, size):         return s // size, s % size      def coords_to_state(x, y, size):         return x * size + y      # Get the state transition probabilities     for s in range(n_states):         x, y = state_to_coords(s, grid_size)          # Up         if x &gt; 0:             sas[s, 0, coords_to_state(x - 1, y, grid_size)] = 1.0         else:             sas[s, 0, s] = 1.0  # Bump into wall          # Down         if x &lt; grid_size - 1:             sas[s, 1, coords_to_state(x + 1, y, grid_size)] = 1.0         else:             sas[s, 1, s] = 1.0  # Bump into wall          # Left         if y &gt; 0:             sas[s, 2, coords_to_state(x, y - 1, grid_size)] = 1.0         else:             sas[s, 2, s] = 1.0  # Bump into wall          # Right         if y &lt; grid_size - 1:             sas[s, 3, coords_to_state(x, y + 1, grid_size)] = 1.0         else:             sas[s, 3, s] = 1.0  # Bump into wall          # Stay         sas[s, 4, s] = 1.0      return reward_function, features, sas, discount, n_states, n_actions In\u00a0[\u00a0]: Copied! <pre># Get Gridworld setup\nreward_function, features, sas, discount, n_states, n_actions = (\n    create_gridworld()\n)\n\n# Plot\nplt.imshow(reward_function.reshape(15, 15))\n</pre> # Get Gridworld setup reward_function, features, sas, discount, n_states, n_actions = (     create_gridworld() )  # Plot plt.imshow(reward_function.reshape(15, 15)) Out[\u00a0]: <pre>&lt;matplotlib.image.AxesImage at 0x7f86f845a5f0&gt;</pre> <p>And another with a large negative area.</p> In\u00a0[\u00a0]: Copied! <pre>reward_function2 = reward_function.copy()\nreward_function2[4:15] = -5\nreward_function2[19:30] = -5\nreward_function2[34:45] = -5\nreward_function2[49:60] = -5\nreward_function2[64:75] = -5\nreward_function2[79:90] = -5\n\nplt.imshow(reward_function2.reshape(15, 15))\n</pre> reward_function2 = reward_function.copy() reward_function2[4:15] = -5 reward_function2[19:30] = -5 reward_function2[34:45] = -5 reward_function2[49:60] = -5 reward_function2[64:75] = -5 reward_function2[79:90] = -5  plt.imshow(reward_function2.reshape(15, 15)) Out[\u00a0]: <pre>&lt;matplotlib.image.AxesImage at 0x7f86c87ec9d0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Parameters for the value iteration\nmax_iter = 1000\ntol = 1e-4\n\n# Solve\nvalues, q_values = solve_value_iteration(\n    sas.shape[0],\n    sas.shape[1],\n    jnp.array(reward_function),\n    max_iter,\n    discount,\n    jnp.array(sas),\n    tol,\n)\nvalues2, q_values2 = solve_value_iteration(\n    sas.shape[0],\n    sas.shape[1],\n    jnp.array(reward_function2),\n    max_iter,\n    discount,\n    jnp.array(sas),\n    tol,\n)\n</pre> # Parameters for the value iteration max_iter = 1000 tol = 1e-4  # Solve values, q_values = solve_value_iteration(     sas.shape[0],     sas.shape[1],     jnp.array(reward_function),     max_iter,     discount,     jnp.array(sas),     tol, ) values2, q_values2 = solve_value_iteration(     sas.shape[0],     sas.shape[1],     jnp.array(reward_function2),     max_iter,     discount,     jnp.array(sas),     tol, ) In\u00a0[31]: Copied! <pre>f, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# get grid size\ngrid_size = int(np.sqrt(n_states))\n\nax[0].imshow(values.reshape(grid_size, grid_size), interpolation=\"nearest\")\nax[1].imshow(values2.reshape(grid_size, grid_size), interpolation=\"nearest\")\n</pre> f, ax = plt.subplots(1, 2, figsize=(10, 5))  # get grid size grid_size = int(np.sqrt(n_states))  ax[0].imshow(values.reshape(grid_size, grid_size), interpolation=\"nearest\") ax[1].imshow(values2.reshape(grid_size, grid_size), interpolation=\"nearest\") Out[31]: <pre>&lt;matplotlib.image.AxesImage at 0x7f86c81425f0&gt;</pre>"},{"location":"examples/planning/value_iteration/#value-iteration-example","title":"Value iteration example\u00b6","text":"<p>Value iteration is a dynamic programming algorithm that computes the optimal value function and the optimal policy for a Markov decision process (MDP). It is a \"model-based\" algorithm, meaning that it requires knowledge of the transition probabilities and rewards of the MDP.</p>"},{"location":"examples/planning/value_iteration/#imports","title":"Imports\u00b6","text":""},{"location":"examples/planning/value_iteration/#create-a-function-to-define-a-simple-grid-world-environment","title":"Create a function to define a simple grid world environment\u00b6","text":""},{"location":"examples/planning/value_iteration/#plot-the-gridworld-environment","title":"Plot the gridworld environment\u00b6","text":"<p>We'll do one with a single positive goal state.</p>"},{"location":"examples/planning/value_iteration/#run-the-value-iteration-algorithm","title":"Run the value iteration algorithm\u00b6","text":""},{"location":"examples/planning/value_iteration/#plot-the-value-fuction","title":"Plot the value fuction\u00b6","text":""},{"location":"reference/decision_rules/","title":"Decision Rules","text":""},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules","title":"decision_rules","text":"<p>Functions:</p> <ul> <li> <code>softmax</code>             \u2013              <p>Softmax function, with optional temperature parameter.</p> </li> <li> <code>softmax_inverse_temperature</code>             \u2013              <p>Softmax function, with optional inverse temperature parameter.</p> </li> <li> <code>softmax_stickiness</code>             \u2013              <p>Softmax function with choice stickiness, and optional temperature</p> </li> <li> <code>softmax_stickiness_inverse_temperature</code>             \u2013              <p>Softmax function with choice stickiness, and optional inverse temperature</p> </li> <li> <code>softmax_subtract_max</code>             \u2013              <p>Softmax function, with optional temperature parameter.</p> </li> </ul>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax","title":"softmax","text":"<pre><code>softmax(value: ArrayLike, temperature: float = 1) -&gt; ArrayLike\n</code></pre> <p>Softmax function, with optional temperature parameter.</p> <p>In equation form, this is:</p> \\[ P(a) = \\frac{e^{Q(a) / \\tau}}{\\sum_{b} e^{Q(b) / \\tau}} \\] <p>Where <code>P(a)</code> is the probability of choosing action <code>a</code>, <code>Q(a)</code> is the value of action <code>a</code>, and <code>au</code> is the temperature parameter.</p> <p>Note that the value of the temperature parameter will depend on the range of the values of the Q function.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ArrayLike</code> (              <code>ArrayLike</code> )          \u2013            <p>Choice probabilities, of shape (n_trials, n_bandits)</p> </li> </ul> Source code in <code>behavioural_modelling/decision_rules.py</code> <pre><code>@jax.jit\ndef softmax(value: ArrayLike, temperature: float = 1) -&gt; ArrayLike:\n    \"\"\"\n    Softmax function, with optional temperature parameter.\n\n    In equation form, this is:\n\n    $$\n    P(a) = \\\\frac{e^{Q(a) / \\\\tau}}{\\\\sum_{b} e^{Q(b) / \\\\tau}}\n    $$\n\n    Where `P(a)` is the probability of choosing action `a`,\n    `Q(a)` is the value of action `a`, and `\\tau` is the\n    temperature parameter.\n\n    Note that the value of the temperature parameter will\n    depend on the range of the values of the Q function.\n\n    Args:\n        value (ArrayLike): Array of values to apply softmax to, of shape\n            (n_trials, n_bandits)\n        temperature (float, optional): Softmax temperature, in range [0, inf].\n            Note that this is temperature rather than inverse temperature;\n            values are divided by this value. Higher values make choices less\n            deterministic. Defaults to 1.\n\n    Returns:\n        ArrayLike: Choice probabilities, of shape (n_trials, n_bandits)\n    \"\"\"\n\n    return (jnp.exp(value / temperature)) / (\n        jnp.sum(jnp.exp(value / temperature), axis=1)[:, None]\n    )\n</code></pre>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Array of values to apply softmax to, of shape (n_trials, n_bandits)</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Softmax temperature, in range [0, inf]. Note that this is temperature rather than inverse temperature; values are divided by this value. Higher values make choices less deterministic. Defaults to 1.</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_inverse_temperature","title":"softmax_inverse_temperature","text":"<pre><code>softmax_inverse_temperature(value: ArrayLike, inverse_temperature: float = 1) -&gt; ArrayLike\n</code></pre> <p>Softmax function, with optional inverse temperature parameter.</p> <p>In equation form, this is:</p> \\[ P(a) = \\frac{e^{\\beta \\cdot Q(a)}}{\\sum_{b} e^{\\beta \\cdot Q(b)}} \\] <p>Where <code>P(a)</code> is the probability of choosing action <code>a</code>, <code>Q(a)</code> is the value of action <code>a</code>, and <code>beta</code> is the inverse temperature parameter.</p> <p>Note that the value of the inverse temperature parameter will depend on the range of the values of the Q function.</p> <p>Parameters:</p> Source code in <code>behavioural_modelling/decision_rules.py</code> <pre><code>@jax.jit\ndef softmax_inverse_temperature(\n    value: ArrayLike, inverse_temperature: float = 1\n) -&gt; ArrayLike:\n    \"\"\"\n    Softmax function, with optional inverse temperature parameter.\n\n    In equation form, this is:\n\n    $$\n    P(a) = \\\\frac{e^{\\\\beta \\\\cdot Q(a)}}{\\\\sum_{b} e^{\\\\beta \\\\cdot Q(b)}}\n    $$\n\n    Where `P(a)` is the probability of choosing action `a`,\n    `Q(a)` is the value of action `a`, and `beta` is the\n    inverse temperature parameter.\n\n    Note that the value of the inverse temperature parameter will\n    depend on the range of the values of the Q function.\n\n    Args:\n        value (ArrayLike): Array of values to apply softmax to, of shape\n            (n_trials, n_bandits)\n        inverse_temperature (float, optional): Softmax inverse temperature, in\n            range [0, inf]. Note that this is inverse temperature rather than\n            temperature; values are multiplied by this value. Higher values\n            make choices more deterministic. Defaults to 1.\n    \"\"\"\n    return (jnp.exp(inverse_temperature * value)) / (\n        jnp.sum(jnp.exp(inverse_temperature * value), axis=1)[:, None]\n    )\n</code></pre>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_inverse_temperature(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Array of values to apply softmax to, of shape (n_trials, n_bandits)</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_inverse_temperature(inverse_temperature)","title":"<code>inverse_temperature</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Softmax inverse temperature, in range [0, inf]. Note that this is inverse temperature rather than temperature; values are multiplied by this value. Higher values make choices more deterministic. Defaults to 1.</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness","title":"softmax_stickiness","text":"<pre><code>softmax_stickiness(value: ArrayLike, temperature: float = 1.0, stickiness: float = 0.0, prev_choice: Optional[ArrayLike] = None) -&gt; ArrayLike\n</code></pre> <p>Softmax function with choice stickiness, and optional temperature parameter.</p> <p>The standard softmax function is:</p> \\[ P(a) = \\frac{e^{Q(a) / \\tau}}{\\sum_{b} e^{Q(b) / \\tau}} \\] <p>With stickiness added:</p> \\[ P(a) = \\frac{e^{(Q(a) + \\kappa \\cdot same(a, a_{t-1}))/\\tau}} {\\sum_{b} e^{(Q(b) + \\kappa \\cdot same(b, a_{t-1}))/\\tau}} \\] <ul> <li>\\(P(a)\\) is the probability of choosing action \\(a\\)</li> <li>\\(Q(a)\\) is the value of action \\(a\\)</li> <li>\\(\\beta\\) is the temperature parameter</li> <li>\\(\\kappa\\) is the stickiness parameter</li> <li>\\(same(a, a_{t-1})\\) is 1 if \\(a\\) matches the previous choice, 0 otherwise</li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArrayLike</code> (              <code>ArrayLike</code> )          \u2013            <p>Choice probabilities, shape (n_trials, n_bandits)</p> </li> </ul> Source code in <code>behavioural_modelling/decision_rules.py</code> <pre><code>@jax.jit\ndef softmax_stickiness(\n    value: ArrayLike,\n    temperature: float = 1.0,\n    stickiness: float = 0.0,\n    prev_choice: Optional[ArrayLike] = None,\n) -&gt; ArrayLike:\n    \"\"\"\n    Softmax function with choice stickiness, and optional temperature\n    parameter.\n\n    The standard softmax function is:\n\n    $$\n    P(a) = \\\\frac{e^{Q(a) / \\\\tau}}{\\\\sum_{b} e^{Q(b) / \\\\tau}}\n    $$\n\n    With stickiness added:\n\n    $$\n    P(a) = \\\\frac{e^{(Q(a) + \\\\kappa \\\\cdot same(a, a_{t-1}))/\\\\tau}}\n    {\\\\sum_{b} e^{(Q(b) + \\\\kappa \\\\cdot same(b, a_{t-1}))/\\\\tau}}\n    $$\n\n    - $P(a)$ is the probability of choosing action $a$\n    - $Q(a)$ is the value of action $a$\n    - $\\\\beta$ is the temperature parameter\n    - $\\kappa$ is the stickiness parameter\n    - $same(a, a_{t-1})$ is 1 if $a$ matches the previous choice, 0 otherwise\n\n    Args:\n        value (ArrayLike): Array of values to apply softmax to, shape\n            `(n_trials, n_bandits)`. \n            Note that this **does not**\n            account for trial-wise dependencies, so each trial is treated\n            independently (i.e., we use precomputed choices, therefore the \n            precomputed choice on trial `t-1` can influence the choice on \n            trial `t`, but this altered choice likelihood on trial `t` will not\n            affect any subsequent trials since we rely on the precomputed\n            choices provided).\n            This can be useful to apply the same stickiness to\n            all trials, but additional code will be required to account for\n            trial-wise dependencies (i.e., the choice on trial `t-1`)\n            influencing the choice on trial `t`, and this subsequently\n            influencing trials `t+1` etc.).\n        temperature (float, optional): Softmax temperature, in range [0, inf].\n            Note that this is temperature rather than inverse temperature;\n            values are divided by this value. Higher values\n            make choices less deterministic. Defaults to 1.0.\n        stickiness (float, optional): Weight given to previous choices, range\n            (-inf, inf). Positive values increase probability of repeating\n            choices. Defaults to 0.0\n        prev_choice (ArrayLike, optional): One-hot encoded previous choices,\n            shape (n_trials, n_bandits). Defaults to None.\n\n    Returns:\n        ArrayLike: Choice probabilities, shape (n_trials, n_bandits)\n    \"\"\"\n\n    sticky_value = value + stickiness * prev_choice\n\n    return (jnp.exp(sticky_value / temperature)) / (\n        jnp.sum(jnp.exp(sticky_value / temperature), axis=1)[:, None]\n    )\n</code></pre>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Array of values to apply softmax to, shape <code>(n_trials, n_bandits)</code>.  Note that this does not account for trial-wise dependencies, so each trial is treated independently (i.e., we use precomputed choices, therefore the  precomputed choice on trial <code>t-1</code> can influence the choice on  trial <code>t</code>, but this altered choice likelihood on trial <code>t</code> will not affect any subsequent trials since we rely on the precomputed choices provided). This can be useful to apply the same stickiness to all trials, but additional code will be required to account for trial-wise dependencies (i.e., the choice on trial <code>t-1</code>) influencing the choice on trial <code>t</code>, and this subsequently influencing trials <code>t+1</code> etc.).</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Softmax temperature, in range [0, inf]. Note that this is temperature rather than inverse temperature; values are divided by this value. Higher values make choices less deterministic. Defaults to 1.0.</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness(stickiness)","title":"<code>stickiness</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Weight given to previous choices, range (-inf, inf). Positive values increase probability of repeating choices. Defaults to 0.0</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness(prev_choice)","title":"<code>prev_choice</code>","text":"(<code>ArrayLike</code>, default:                   <code>None</code> )           \u2013            <p>One-hot encoded previous choices, shape (n_trials, n_bandits). Defaults to None.</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness_inverse_temperature","title":"softmax_stickiness_inverse_temperature","text":"<pre><code>softmax_stickiness_inverse_temperature(value: ArrayLike, inverse_temperature: float = 1.0, stickiness: float = 0.0, prev_choice: Optional[ArrayLike] = None) -&gt; ArrayLike\n</code></pre> <p>Softmax function with choice stickiness, and optional inverse temperature parameter.</p> <p>The standard softmax function is:</p> \\[ P(a) = \\frac{e^{\\beta \\cdot Q(a)}}{\\sum_{b} e^{\\beta \\cdot Q(b)}} \\] <p>With stickiness added:</p> \\[ P(a) = \\frac{e^{(Q(a) + \\kappa \\cdot same(a, a_{t-1}))/\\tau}} {\\sum_{b} e^{(Q(b) + \\kappa \\cdot same(b, a_{t-1}))/\\tau}} \\] <ul> <li>\\(P(a)\\) is the probability of choosing action \\(a\\)</li> <li>\\(Q(a)\\) is the value of action \\(a\\)</li> <li>\\(\\beta\\) is the inverse temperature parameter</li> <li>\\(\\kappa\\) is the stickiness parameter</li> <li>\\(same(a, a_{t-1})\\) is 1 if \\(a\\) matches the previous choice, 0 otherwise</li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArrayLike</code> (              <code>ArrayLike</code> )          \u2013            <p>Choice probabilities, shape (n_trials, n_bandits)</p> </li> </ul> Source code in <code>behavioural_modelling/decision_rules.py</code> <pre><code>@jax.jit\ndef softmax_stickiness_inverse_temperature(\n    value: ArrayLike,\n    inverse_temperature: float = 1.0,\n    stickiness: float = 0.0,\n    prev_choice: Optional[ArrayLike] = None,\n) -&gt; ArrayLike:\n    \"\"\"\n    Softmax function with choice stickiness, and optional inverse temperature\n    parameter.\n\n    The standard softmax function is:\n\n    $$\n    P(a) = \\\\frac{e^{\\\\beta \\\\cdot Q(a)}}{\\\\sum_{b} e^{\\\\beta \\\\cdot Q(b)}}\n    $$\n\n    With stickiness added:\n\n    $$\n    P(a) = \\\\frac{e^{(Q(a) + \\\\kappa \\\\cdot same(a, a_{t-1}))/\\\\tau}}\n    {\\\\sum_{b} e^{(Q(b) + \\\\kappa \\\\cdot same(b, a_{t-1}))/\\\\tau}}\n    $$\n\n    - $P(a)$ is the probability of choosing action $a$\n    - $Q(a)$ is the value of action $a$\n    - $\\\\beta$ is the inverse temperature parameter\n    - $\\kappa$ is the stickiness parameter\n    - $same(a, a_{t-1})$ is 1 if $a$ matches the previous choice, 0 otherwise\n\n    Args:\n        value (ArrayLike): Array of values to apply softmax to, shape\n            `(n_trials, n_bandits)`. \n            Note that this **does not**\n            account for trial-wise dependencies, so each trial is treated\n            independently (i.e., we use precomputed choices, therefore the \n            precomputed choice on trial `t-1` can influence the choice on \n            trial `t`, but this altered choice likelihood on trial `t` will not\n            affect any subsequent trials since we rely on the precomputed\n            choices provided).\n            This can be useful to apply the same stickiness to\n            all trials, but additional code will be required to account for\n            trial-wise dependencies (i.e., the choice on trial `t-1`)\n            influencing the choice on trial `t`, and this subsequently\n            influencing trials `t+1` etc.).\n        inverse_temperature (float, optional): Softmax inverse temperature,\n            range [0, inf]. Higher values make choices more deterministic.\n            Defaults to 1.0\n        stickiness (float, optional): Weight given to previous choices, range\n            (-inf, inf). Positive values increase probability of repeating\n            choices. Defaults to 0.0\n        prev_choice (ArrayLike, optional): One-hot encoded previous choices,\n            shape (n_trials, n_bandits). Defaults to None.\n\n    Returns:\n        ArrayLike: Choice probabilities, shape (n_trials, n_bandits)\n    \"\"\"\n\n    sticky_value = value + stickiness * prev_choice\n\n    return (jnp.exp(inverse_temperature * sticky_value)) / (\n        jnp.sum(jnp.exp(inverse_temperature * sticky_value), axis=1)[:, None]\n    )\n</code></pre>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness_inverse_temperature(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Array of values to apply softmax to, shape <code>(n_trials, n_bandits)</code>.  Note that this does not account for trial-wise dependencies, so each trial is treated independently (i.e., we use precomputed choices, therefore the  precomputed choice on trial <code>t-1</code> can influence the choice on  trial <code>t</code>, but this altered choice likelihood on trial <code>t</code> will not affect any subsequent trials since we rely on the precomputed choices provided). This can be useful to apply the same stickiness to all trials, but additional code will be required to account for trial-wise dependencies (i.e., the choice on trial <code>t-1</code>) influencing the choice on trial <code>t</code>, and this subsequently influencing trials <code>t+1</code> etc.).</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness_inverse_temperature(inverse_temperature)","title":"<code>inverse_temperature</code>","text":"(<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Softmax inverse temperature, range [0, inf]. Higher values make choices more deterministic. Defaults to 1.0</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness_inverse_temperature(stickiness)","title":"<code>stickiness</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Weight given to previous choices, range (-inf, inf). Positive values increase probability of repeating choices. Defaults to 0.0</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_stickiness_inverse_temperature(prev_choice)","title":"<code>prev_choice</code>","text":"(<code>ArrayLike</code>, default:                   <code>None</code> )           \u2013            <p>One-hot encoded previous choices, shape (n_trials, n_bandits). Defaults to None.</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_subtract_max","title":"softmax_subtract_max","text":"<pre><code>softmax_subtract_max(value: ArrayLike, temperature: float = 1) -&gt; ArrayLike\n</code></pre> <p>Softmax function, with optional temperature parameter.</p> <p>Subtracts the maximum value before applying softmax to avoid overflow.</p> <p>In equation form, this is:</p> \\[ P(a) = \\frac{e^{(Q(a) - \\max_{b} Q(b)) / \\tau}} {\\sum_{b} e^{(Q(b) - \\max_{c} Q(c)) / \\tau}} \\] <p>Where <code>P(a)</code> is the probability of choosing action <code>a</code>, <code>Q(a)</code> is the value of action <code>a</code>, and <code>au</code> is the temperature parameter.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ArrayLike</code> (              <code>ArrayLike</code> )          \u2013            <p>Choice probabilities, of shape (n_trials, n_bandits)</p> </li> </ul> Source code in <code>behavioural_modelling/decision_rules.py</code> <pre><code>@jax.jit\ndef softmax_subtract_max(\n    value: ArrayLike, temperature: float = 1\n) -&gt; ArrayLike:\n    \"\"\"\n    Softmax function, with optional temperature parameter.\n\n    Subtracts the maximum value before applying softmax to avoid overflow.\n\n    In equation form, this is:\n\n    $$\n    P(a) = \\\\frac{e^{(Q(a) - \\max_{b} Q(b)) / \\\\tau}}\n    {\\\\sum_{b} e^{(Q(b) - \\max_{c} Q(c)) / \\\\tau}}\n    $$\n\n    Where `P(a)` is the probability of choosing action `a`,\n    `Q(a)` is the value of action `a`, and `\\tau` is the\n    temperature parameter.\n\n    Args:\n        value (ArrayLike): Array of values to apply softmax to, of shape\n            (n_trials, n_bandits)\n        temperature (float, optional): Softmax temperature, in range [0, inf].\n            Note that this is temperature rather than inverse temperature;\n            values are divided by this value. Defaults to 1.\n\n    Returns:\n        ArrayLike: Choice probabilities, of shape (n_trials, n_bandits)\n    \"\"\"\n    # Subtract max value to avoid overflow\n    return (jnp.exp((value - value.max(axis=1)[:, None]) / temperature)) / (\n        jnp.sum(\n            jnp.exp((value - value.max(axis=1)[:, None]) / temperature), axis=1\n        )[:, None]\n    )\n</code></pre>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_subtract_max(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Array of values to apply softmax to, of shape (n_trials, n_bandits)</p>"},{"location":"reference/decision_rules/#behavioural_modelling.decision_rules.softmax_subtract_max(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Softmax temperature, in range [0, inf]. Note that this is temperature rather than inverse temperature; values are divided by this value. Defaults to 1.</p>"},{"location":"reference/learning/","title":"Learning models","text":"<p>This module contains models of learning.</p>"},{"location":"reference/learning/#behavioural_modelling.learning","title":"learning","text":"<p>Modules:</p> <ul> <li> <code>beta_models</code>           \u2013            </li> <li> <code>rescorla_wagner</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>asymmetric_volatile_rescorla_wagner_update</code>             \u2013              <p>Updates the estimated value of a state or action using a variant</p> </li> <li> <code>average_betas</code>             \u2013              <p>Average two beta distributions, weighted by W.</p> </li> <li> <code>beta_mean_var</code>             \u2013              <p>Calculate mean and variance of a beta distribution.</p> </li> <li> <code>choice_from_action_p</code>             \u2013              <p>Choose an action from a set of action probabilities. Can take probabilities</p> </li> <li> <code>complement_counterfactual</code>             \u2013              <p>Counterfactual function that sets the value of unchosen actions to the</p> </li> <li> <code>generalised_beta_mean_var</code>             \u2013              <p>Calculate mean and variance of a generalised beta distribution.</p> </li> <li> <code>leaky_beta_update</code>             \u2013              <p>Update estimates using the (asymmetric) leaky beta model. </p> </li> <li> <code>multiply_beta_by_scalar</code>             \u2013              <p>Multiply a beta distribution by a scalar.</p> </li> <li> <code>softmax</code>             \u2013              <p>Softmax function, with optional temperature parameter.</p> </li> <li> <code>softmax_stickiness</code>             \u2013              <p>Softmax function with choice stickiness, and optional temperature</p> </li> <li> <code>sum_betas</code>             \u2013              <p>Sum two beta distributions. This uses an approximation described in the following paper:</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>asymmetric_rescorla_wagner_update</code>               (<code>Tuple[ArrayLike, ArrayLike]</code>)           \u2013            <p>Updates the estimated value of a state or action using the Asymmetric</p> </li> <li> <code>asymmetric_rescorla_wagner_update_choice</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using the asymmetric Rescorla-Wagner</p> </li> <li> <code>asymmetric_rescorla_wagner_update_choice_sticky</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using the asymmetric Rescorla-Wagner</p> </li> <li> <code>asymmetric_volatile_dynamic_rescorla_wagner_update_choice</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using a variant of the Rescorla-Wagner</p> </li> <li> <code>asymmetric_volatile_rescorla_wagner_single_value_update_choice</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using the asymmetric volatile dynamic</p> </li> </ul>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update","title":"asymmetric_rescorla_wagner_update  <code>module-attribute</code>","text":"<pre><code>asymmetric_rescorla_wagner_update: Tuple[ArrayLike, ArrayLike] = jit(asymmetric_rescorla_wagner_update, static_argnames='counterfactual_value')\n</code></pre> <p>Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule.</p> <p>The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative.</p> <p>Value estimates are only updated for chosen actions. For unchosen actions, the prediction error is set to 0.</p> <p>Counterfactual updating can be used to set the value of unchosen actions according to a function of the value of chosen actions. This can be useful in cases where the value of unchosen actions should be set to a specific value, such as the negative of the value of chosen actions. By default this function sets the value of unchosen actions to the complement of the value of chosen actions:</p> <pre><code>counterfactual_value: callable = lambda reward, chosen: jnp.where(\n    chosen == 1, \n    0.0, \n    1.0 - jnp.sum(reward * jnp.asarray(chosen == 1, dtype=reward.dtype))\n)\n</code></pre> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ArrayLike, ArrayLike]</code>           \u2013            <p>Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]: The updated value and the prediction error.</p> </li> </ul>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current estimated value of a state or action.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update(alpha_p)","title":"<code>alpha_p</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The learning rate used when the prediction error is positive.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update(alpha_n)","title":"<code>alpha_n</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The learning rate used when the prediction error is negative.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update(counterfactual_value)","title":"<code>counterfactual_value</code>","text":"(<code>callable]</code>)           \u2013            <p>The value to use for unchosen actions. This should be provided as a callable function that returns a value. This will have no effect if <code>update_all_options</code> is set to False. The function takes as input the values of <code>outcome</code> and <code>chosen</code> (i.e., the two elements of the <code>outcome_chosen</code> argument). By default, this assumes that outcomes are binary and sets the value of unchosen actions to the complement of the value of chosen actions.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update(update_all_options)","title":"<code>update_all_options</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to update the value estimates for all options, regardless of whether they were chosen. Defaults to False.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice","title":"asymmetric_rescorla_wagner_update_choice  <code>module-attribute</code>","text":"<pre><code>asymmetric_rescorla_wagner_update_choice: Array = jit(asymmetric_rescorla_wagner_update_choice, static_argnums=(5, 6))\n</code></pre> <p>Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function.</p> <p>See <code>asymmetric_rescorla_wagner_update</code> for details on the learning rule.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Tuple[jax.Array, Tuple[jax.Array, jax.Array, int, jax.Array]]: - updated_value (jax.Array): The updated value estimate. - output_tuple (Tuple[jax.Array, jax.Array, int,     jax.Array]):     - value (jax.Array): The original value estimate.     - choice_p (jax.Array): The choice probabilities.     - choice (int): The chosen action.     - choice_array (jax.Array): The chosen action in one-hot         format.</p> </li> </ul>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current value estimate.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(outcome_key)","title":"<code>outcome_key</code>","text":"(<code>Tuple[ArrayLike, PRNGKey]</code>)           \u2013            <p>A tuple containing the outcome and the PRNG key.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(alpha_p)","title":"<code>alpha_p</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for positive outcomes.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(alpha_n)","title":"<code>alpha_n</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for negative outcomes.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(counterfactual_value)","title":"<code>counterfactual_value</code>","text":"(<code>callable]</code>)           \u2013            <p>The value to use for unchosen actions. This should be provided as a callable function that returns a value. This will have no effect if <code>update_all_options</code> is set to False. The function takes as input the values of <code>outcome</code> and <code>chosen</code> (i.e., the two elements of the <code>outcome_chosen</code> argument). By default, this assumes that outcomes are binary and sets the value of unchosen actions to the complement of the value of chosen actions.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice(update_all_options)","title":"<code>update_all_options</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to update the value estimates for all options, regardless of whether they were</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky","title":"asymmetric_rescorla_wagner_update_choice_sticky  <code>module-attribute</code>","text":"<pre><code>asymmetric_rescorla_wagner_update_choice_sticky: Array = jit(asymmetric_rescorla_wagner_update_choice_sticky, static_argnums=(6, 7))\n</code></pre> <p>Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function.</p> <p>Incorporates additional choice stickiness parameter, such that the probability of choosing the same option as the previous trial is increased (or decreased if the value is negative).</p> <p>See <code>asymmetric_rescorla_wagner_update</code> for details on the learning rule.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Tuple[jax.Array, Tuple[jax.Array, jax.Array, int, jax.Array]]: - updated_value (jax.Array): The updated value estimate. - output_tuple (Tuple[jax.Array, jax.Array, int,     jax.Array]):     - value (jax.Array): The original value estimate.     - choice_p (jax.Array): The choice probabilities.     - choice (int): The chosen action.     - choice_array (jax.Array): The chosen action in one-hot         format.</p> </li> </ul>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(value_choice)","title":"<code>value_choice</code>","text":"(<code>Tuple[ArrayLike, ArrayLike]</code>)           \u2013            <p>A tuple containing the current value estimate and the previous choice. The previous choice should be a one-hot encoded array of shape (n_actions,) where 1 indicates the chosen action and 0 indicates the unchosen actions.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(outcome_key)","title":"<code>outcome_key</code>","text":"(<code>Tuple[ArrayLike, PRNGKey]</code>)           \u2013            <p>A tuple containing the outcome and the PRNG key.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(alpha_p)","title":"<code>alpha_p</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for positive outcomes.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(alpha_n)","title":"<code>alpha_n</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for negative outcomes.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(stickiness)","title":"<code>stickiness</code>","text":"(<code>float</code>)           \u2013            <p>The stickiness parameter for softmax function.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(counterfactual_value)","title":"<code>counterfactual_value</code>","text":"(<code>callable]</code>)           \u2013            <p>The value to use for unchosen actions. This should be provided as a callable function that returns a value. This will have no effect if <code>update_all_options</code> is set to False. The function takes as input the values of <code>outcome</code> and <code>chosen</code> (i.e., the two elements of the <code>outcome_chosen</code> argument). Defaults to <code>lambda x, y: (1 - x) * (1 - y)</code>, which assumes outcomes are binary (0 or 1), and sets the value of unchosen actions to complement the value of chosen actions (i.e., a chosen value of 1 will set the unchosen value to 0 and vice versa).</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_rescorla_wagner_update_choice_sticky(update_all_options)","title":"<code>update_all_options</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to update the value estimates for all options, regardless of whether they were</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice","title":"asymmetric_volatile_dynamic_rescorla_wagner_update_choice  <code>module-attribute</code>","text":"<pre><code>asymmetric_volatile_dynamic_rescorla_wagner_update_choice: Array = jit(asymmetric_volatile_dynamic_rescorla_wagner_update_choice, static_argnums=(7,))\n</code></pre> <p>Updates the value estimate using a variant of the Rescorla-Wagner learning rule that adjusts learning rate based on volatility and prediction error sign, and chooses an option based on the softmax function.</p> <p>Note that learning rates for this function are transformed using a sigmoid function to ensure they are between 0 and 1. The raw parameter values supplied to the function must therefore be unbounded.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Tuple[jax.Array, Tuple[jax.typing.ArrayLike, jax.Array, int, jax.Array]]: - updated_value (jax.Array): The updated value estimate. - output_tuple (Tuple[jax.Array, jax.Array, int,     jax.Array]):     - value (jax.Array): The original value estimate.     - choice_p (jax.Array): The choice probabilities.     - choice (int): The chosen action.     - choice_array (jax.Array): The chosen action in one-hot         format.</p> </li> </ul>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current value estimate.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_base)","title":"<code>alpha_base</code>","text":"(<code>float</code>)           \u2013            <p>The base learning rate.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_volatility)","title":"<code>alpha_volatility</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for volatile outcomes.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_pos_neg)","title":"<code>alpha_pos_neg</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for positive and negative prediction errors.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_interaction)","title":"<code>alpha_interaction</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for the interaction between volatility and prediction error sign.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice","title":"asymmetric_volatile_rescorla_wagner_single_value_update_choice  <code>module-attribute</code>","text":"<pre><code>asymmetric_volatile_rescorla_wagner_single_value_update_choice: Array = jit(asymmetric_volatile_rescorla_wagner_single_value_update_choice)\n</code></pre> <p>Updates the value estimate using the asymmetric volatile dynamic Rescorla-Wagner algorithm, and chooses an option based on the softmax function.</p> <p>This version of the function is designed for cases where the a single value is being learnt, and this value is used to determine which of two options to choose. In practice, the value of option 1 is learnt, and the value of option 2 is set to 1 - value. This is appropriate for cases where the value of one option is the complement of the other.</p> <p>Note that learning rates for this function are transformed using a sigmoid function to ensure they are between 0 and 1. The raw parameter values supplied to the function must therefore be unbounded.</p> <p>Parameters:</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current value estimate.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_base)","title":"<code>alpha_base</code>","text":"(<code>float</code>)           \u2013            <p>The base learning rate.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_volatility)","title":"<code>alpha_volatility</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for volatile outcomes.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_pos_neg)","title":"<code>alpha_pos_neg</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for positive and negative prediction errors.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_interaction)","title":"<code>alpha_interaction</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for the interaction between volatility and prediction error sign.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_single_value_update_choice(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_update","title":"asymmetric_volatile_rescorla_wagner_update","text":"<pre><code>asymmetric_volatile_rescorla_wagner_update(value: ArrayLike, outcome_chosen_volatility: Tuple[ArrayLike, ArrayLike, ArrayLike], alpha_base: float, alpha_volatility: float, alpha_pos_neg: float, alpha_interaction: float) -&gt; Tuple[ArrayLike, Tuple[ArrayLike, ArrayLike]]\n</code></pre> <p>Updates the estimated value of a state or action using a variant of the Rescorla-Wagner learning rule that incorporates adjusting the learning rate based on both volatility and prediction error sign.</p> <p>Note that learning rates for this function are transformed using a sigmoid function to ensure they are between 0 and 1. The raw parameter values supplied to the function must therefore be unbounded.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ArrayLike, Tuple[ArrayLike, ArrayLike]]</code>           \u2013            <p>Tuple[jax.typing.ArrayLike, Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]]: - updated_value (jax.typing.ArrayLike): The updated value estimate. - output_tuple (Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]):     - value (jax.typing.ArrayLike): The original value estimate.     - prediction_error (jax.typing.ArrayLike): The prediction       error.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/rescorla_wagner.py</code> <pre><code>@jax.jit\ndef asymmetric_volatile_rescorla_wagner_update(\n    value: jax.typing.ArrayLike,\n    outcome_chosen_volatility: Tuple[\n        jax.typing.ArrayLike,\n        jax.typing.ArrayLike,\n        jax.typing.ArrayLike,\n    ],\n    alpha_base: float,\n    alpha_volatility: float,\n    alpha_pos_neg: float,\n    alpha_interaction: float,\n) -&gt; Tuple[\n    jax.typing.ArrayLike, Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]\n]:\n    \"\"\"\n    Updates the estimated value of a state or action using a variant\n    of the Rescorla-Wagner learning rule that incorporates adjusting\n    the learning rate based on both volatility and prediction error sign.\n\n    Note that learning rates for this function are transformed using a\n    sigmoid function to ensure they are between 0 and 1. The raw\n    parameter values supplied to the function must therefore be\n    unbounded.\n\n    Args:\n        value (jax.typing.ArrayLike): The current estimated value of a\n            state or action.\n        outcome_chosen_volatility (Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike,\n            jax.typing.ArrayLike]): A tuple containing the outcome, the chosen\n            action, and the volatility indicator. The outcome is a float or an\n            array (e.g., for a single outcome or multiple outcomes). The chosen\n            action is a one-hot encoded array of shape (n_actions,) where 1\n            indicates the chosen action and 0 indicates the unchosen actions.\n            The volatility indicator is a binary value that indicates whether\n            the outcome is volatile (1) or stable (0).\n        alpha_base (float): The base learning rate.\n        alpha_volatility (float): The learning rate adjustment for volatile\n            outcomes.\n        alpha_pos_neg (float): The learning rate adjustment for positive and\n            negative prediction errors.\n        alpha_interaction (float): The learning rate adjustment for the\n            interaction between volatility and prediction error sign.\n\n    Returns:\n        Tuple[jax.typing.ArrayLike, Tuple[jax.typing.ArrayLike,\n            jax.typing.ArrayLike]]:\n            - updated_value (jax.typing.ArrayLike): The updated value estimate.\n            - output_tuple (Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]):\n                - value (jax.typing.ArrayLike): The original value estimate.\n                - prediction_error (jax.typing.ArrayLike): The prediction\n                  error.\n    \"\"\"\n\n    # Unpack the outcome and the chosen action\n    outcome, chosen, volatility_indicator = outcome_chosen_volatility\n\n    # Calculate the prediction error\n    prediction_error = outcome - value\n\n    # Set prediction error to 0 for unchosen actions\n    prediction_error = prediction_error * chosen\n\n    # Determine whether the error is positive (1) or negative (-1)\n    PE_sign = jnp.sign(prediction_error)\n\n    # Compute interaction term (volatility_indicator * error_sign)\n    interaction_term = volatility_indicator * PE_sign\n\n    # Compute the dynamic learning rate using base, volatility, and interaction terms\n    # Remember we can't use if else statements here because JAX doesn't tolerate them\n    # Use adjusted learning rates for positive/negative prediction errors\n    alpha_t = jax.nn.sigmoid(\n        alpha_base\n        + alpha_volatility * volatility_indicator\n        + alpha_pos_neg * PE_sign\n        + alpha_interaction * interaction_term\n    )\n\n    # Update the value\n    updated_value = value + alpha_t * prediction_error\n\n    return updated_value, (value, prediction_error)\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_update(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current estimated value of a state or action.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_update(alpha_base)","title":"<code>alpha_base</code>","text":"(<code>float</code>)           \u2013            <p>The base learning rate.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_update(alpha_volatility)","title":"<code>alpha_volatility</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for volatile outcomes.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_update(alpha_pos_neg)","title":"<code>alpha_pos_neg</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for positive and negative prediction errors.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.asymmetric_volatile_rescorla_wagner_update(alpha_interaction)","title":"<code>alpha_interaction</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for the interaction between volatility and prediction error sign.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.average_betas","title":"average_betas","text":"<pre><code>average_betas(beta1_params: ArrayLike, beta2_params: ArrayLike, W1: float = 0.5, W2: float = 0.5) -&gt; ndarray\n</code></pre> <p>Average two beta distributions, weighted by W.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>jnp.ndarray: New beta distribution parameters.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef average_betas(\n    beta1_params: ArrayLike,\n    beta2_params: ArrayLike,\n    W1: float = 0.5,\n    W2: float = 0.5,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Average two beta distributions, weighted by W.\n\n    Args:\n        beta1_params (ArrayLike): Parameters of first beta distribution.\n        beta2_params (ArrayLike): Parameters of second beta distribution.\n\n    Returns:\n        jnp.ndarray: New beta distribution parameters.\n    \"\"\"\n\n    # Extract parameters\n    a1 = beta1_params[..., 0]\n    b1 = beta1_params[..., 1]\n    a2 = beta2_params[..., 0]\n    b2 = beta2_params[..., 1]\n\n    # Calculate average\n    a_new = (W1 * a1) + (W2 * a2)\n    b_new = (W1 * b1) + (W2 * b2)\n\n    # Return new parameters\n    return jnp.stack([a_new, b_new], axis=-1)\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.average_betas(beta1_params)","title":"<code>beta1_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of first beta distribution.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.average_betas(beta2_params)","title":"<code>beta2_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of second beta distribution.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.beta_mean_var","title":"beta_mean_var","text":"<pre><code>beta_mean_var(beta_params: ArrayLike) -&gt; Tuple[ArrayLike, ArrayLike]\n</code></pre> <p>Calculate mean and variance of a beta distribution.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ArrayLike, ArrayLike]</code>           \u2013            <p>tuple[ArrayLike, ArrayLike]: Mean and variance of the beta distribution.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef beta_mean_var(beta_params: ArrayLike) -&gt; Tuple[ArrayLike, ArrayLike]:\n    \"\"\"\n    Calculate mean and variance of a beta distribution.\n\n    Args:\n        beta_params (ArrayLike): Parameters of the beta distribution. Of shape (n_options, 2),\n        where the first dimension represents the number of options (each of which has its own\n        beta distribution), and the second dimension represents the alpha and beta parameters\n        of each beta distribution.\n\n    Returns:\n        tuple[ArrayLike, ArrayLike]: Mean and variance of the beta distribution.\n    \"\"\"\n    a, b = beta_params[..., 0], beta_params[..., 1]\n    mean = a / (a + b)\n    var = (a * b) / ((a + b) ** 2 * (a + b + 1))\n    return mean, var\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.beta_mean_var(beta_params)","title":"<code>beta_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of the beta distribution. Of shape (n_options, 2),</p>"},{"location":"reference/learning/#behavioural_modelling.learning.choice_from_action_p","title":"choice_from_action_p","text":"<pre><code>choice_from_action_p(key: PRNGKey, probs: ArrayLike, lapse: float = 0.0) -&gt; int\n</code></pre> <p>Choose an action from a set of action probabilities. Can take probabilities in the form of an n-dimensional array, where the last dimension is the number of actions.</p> <p>Noise is added to the choice, with probability <code>lapse</code>. This means that on \"lapse\" trials, the subject will choose an action uniformly at random.</p> <p>Parameters:</p> <p>Returns:     int: Chosen action</p> Source code in <code>behavioural_modelling/utils.py</code> <pre><code>@jax.jit\ndef choice_from_action_p(key: jax.random.PRNGKey, probs: ArrayLike, lapse: float = 0.0) -&gt; int:\n    \"\"\"\n    Choose an action from a set of action probabilities. Can take probabilities\n    in the form of an n-dimensional array, where the last dimension is the\n    number of actions.\n\n    Noise is added to the choice, with probability `lapse`. This means that\n    on \"lapse\" trials, the subject will choose an action uniformly at random.\n\n    Args:\n        key (int): Jax random key\n        probs (np.ndarray): N-dimension array of action probabilities, of shape (..., n_actions)\n        lapse (float, optional): Probability of lapse. Defaults to 0.0.\n    Returns:\n        int: Chosen action\n    \"\"\"\n\n    # Reshape probs\n    probs_reshaped = probs.reshape((-1, probs.shape[-1]))\n\n    # Split keys so that we have one for each index in the first\n    # dimension of probs\n    keys = jax.random.split(key, probs_reshaped.shape[0])\n\n    # Get choices\n    choices = choice_func_vmap(keys, probs_reshaped, lapse)\n\n    # Reshape choices\n    choices = choices.reshape(probs.shape[:-1])\n\n    return choices\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.choice_from_action_p(key)","title":"<code>key</code>","text":"(<code>int</code>)           \u2013            <p>Jax random key</p>"},{"location":"reference/learning/#behavioural_modelling.learning.choice_from_action_p(probs)","title":"<code>probs</code>","text":"(<code>ndarray</code>)           \u2013            <p>N-dimension array of action probabilities, of shape (..., n_actions)</p>"},{"location":"reference/learning/#behavioural_modelling.learning.choice_from_action_p(lapse)","title":"<code>lapse</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Probability of lapse. Defaults to 0.0.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.complement_counterfactual","title":"complement_counterfactual","text":"<pre><code>complement_counterfactual(reward, chosen)\n</code></pre> <p>Counterfactual function that sets the value of unchosen actions to the complement of the value of chosen actions.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>jax.typing.ArrayLike: The counterfactual value.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/rescorla_wagner.py</code> <pre><code>def complement_counterfactual(reward, chosen):\n    \"\"\"\n    Counterfactual function that sets the value of unchosen actions to the\n    complement of the value of chosen actions.\n\n    Args:\n        reward (jax.typing.ArrayLike): The reward received.\n        chosen (jax.typing.ArrayLike): A binary array indicating which\n            action(s) were chosen.\n\n    Returns:\n        jax.typing.ArrayLike: The counterfactual value.\n    \"\"\"\n    return jnp.where(\n        chosen == 1, \n        0.0, \n        1.0 - jnp.sum(reward * jnp.asarray(chosen == 1, dtype=reward.dtype))\n    )\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.complement_counterfactual(reward)","title":"<code>reward</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The reward received.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.complement_counterfactual(chosen)","title":"<code>chosen</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>A binary array indicating which action(s) were chosen.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.generalised_beta_mean_var","title":"generalised_beta_mean_var","text":"<pre><code>generalised_beta_mean_var(alpha: float, beta: float, a: float, b: float) -&gt; Tuple[float, float]\n</code></pre> <p>Calculate mean and variance of a generalised beta distribution.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[float, float]</code>           \u2013            <p>tuple[float, float]: Mean and variance of the beta distribution.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef generalised_beta_mean_var(\n    alpha: float, beta: float, a: float, b: float\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculate mean and variance of a generalised beta distribution.\n\n    Args:\n        alpha (float): Alpha parameter of the beta distribution.\n        beta (float): Beta parameter of the beta distribution.\n        a (float): Lower bound of the beta distribution.\n        b (float): Upper bound of the beta distribution.\n\n    Returns:\n        tuple[float, float]: Mean and variance of the beta distribution.\n    \"\"\"\n    mean = ((b - a) * alpha) / (alpha + beta) + a\n    var = (alpha * beta * (b - a) ** 2) / ((alpha + beta) ** 2 * (alpha + beta + 1))\n    return mean, var\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.generalised_beta_mean_var(alpha)","title":"<code>alpha</code>","text":"(<code>float</code>)           \u2013            <p>Alpha parameter of the beta distribution.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.generalised_beta_mean_var(beta)","title":"<code>beta</code>","text":"(<code>float</code>)           \u2013            <p>Beta parameter of the beta distribution.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.generalised_beta_mean_var(a)","title":"<code>a</code>","text":"(<code>float</code>)           \u2013            <p>Lower bound of the beta distribution.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.generalised_beta_mean_var(b)","title":"<code>b</code>","text":"(<code>float</code>)           \u2013            <p>Upper bound of the beta distribution.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update","title":"leaky_beta_update","text":"<pre><code>leaky_beta_update(estimate: ArrayLike, choices: ArrayLike, outcome: float, tau_p: float, tau_n: float, decay: float, update: int = 1, increment: int = 1) -&gt; ndarray\n</code></pre> <p>Update estimates using the (asymmetric) leaky beta model. </p> <p>This models represents the probability of the outcome associated with each option (e.g., bandits in a bandit task) as a beta distribution.</p> <p>Values are updated according to the following equations:</p> \\[ A_i^{t+1} = \\lambda \\cdot A_i^{t} + outcome_t \\cdot \\tau^{+} \\] \\[ B_i^{t+1} = \\lambda \\cdot B_i^{t} + (1-outcome_t) \\cdot \\tau^{-} \\] <p>This function also allows for updating to be turned off (i.e., the estimate is not updated at all) and for incrementing to be turned off (i.e., decay is applied, but the outcome is not registered).</p> <p>Only chosen options incremented, but all options decay.</p> <p>Parameters:</p> <p>Returns:     jnp.ndarray: Updated value estimates for this trial, with one entry per bandit.</p> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef leaky_beta_update(\n    estimate: ArrayLike,\n    choices: ArrayLike,\n    outcome: float,\n    tau_p: float,\n    tau_n: float,\n    decay: float,\n    update: int = 1,\n    increment: int = 1,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Update estimates using the (asymmetric) leaky beta model. \n\n    This models represents the probability of the outcome associated with each option (e.g., bandits in a bandit task)\n    as a beta distribution.\n\n    Values are updated according to the following equations:\n\n    $$\n    A_i^{t+1} = \\\\lambda \\\\cdot A_i^{t} + outcome_t \\\\cdot \\\\tau^{+}\n    $$\n\n    $$\n    B_i^{t+1} = \\\\lambda \\\\cdot B_i^{t} + (1-outcome_t) \\\\cdot \\\\tau^{-}\n    $$\n\n    This function also allows for updating to be turned off (i.e., the estimate is not updated at all) and for incrementing\n    to be turned off (i.e., decay is applied, but the outcome is not registered).\n\n    Only chosen options incremented, but all options decay.\n\n    Args:\n        estimate (ArrayLike): Alpha and beta estimates for this trial. Should be an array of shape (n, 2) where\n        the first dimension represents the alpha and beta parameters of the beta distribution and the second\n        dimension represents the number of option.\n        choices (ArrayLike): Choices made in this trial. Should have as many entries as there are options, with\n        zeros for non-chosen options and ones for chosen options (i.e., one-hot encoded).\n        outcomes (float): Observed outcome for this trial.\n        tau_p (float): Update rate for outcomes equal to 1.\n        tau_n (float): Update rate for outcomes equal to 0.\n        decay (float): Decay rate.\n        update (int, optional): Whether to update the estimate. If 0, the estimate is not updated (i.e., no decay is\n        applied, and the outcome of the trial does not affect the outcome). Defaults to 1.\n        increment (int, optional): Whether to increment the estimate. If 0, the estimate is not incremented but\n        decay is applied. Defaults to 1.\n    Returns:\n        jnp.ndarray: Updated value estimates for this trial, with one entry per bandit.\n    \"\"\"\n\n    # For each parameter, we apply the decay to (previous value - 1) so that we are in effect\n    # treating 1 as the baseline value. This is helpful becuase values of &lt; 1 can produce\n    # strange-looking distributions (e.g., with joint peaks at 0 and 1). Keeping values\n    # &gt; 1 ensures that the baseline distribution (ignoring any evidence we've observed)\n    # is a flat distribution between 0 and 1. This also generally aids parameter recovery.\n\n    # Make sure any outcomes &gt; 1 are set to 1\n    outcome = jnp.array(outcome &gt; 0, int)\n\n    # Update alpha\n    update_1 = (\n        1 + (decay * (estimate[:, 0] - 1)) + (tau_p * (choices * outcome) * increment)\n    )\n    estimate = estimate.at[:, 0].set(\n        (update * update_1) + ((1 - update) * estimate[:, 0])\n    )\n\n    # Update beta\n    update_2 = (\n        1\n        + (decay * (estimate[:, 1] - 1))\n        + (tau_n * (choices * (1 - outcome)) * increment)\n    )\n    estimate = estimate.at[:, 1].set(\n        (update * update_2) + ((1 - update) * estimate[:, 1])\n    )\n\n    return estimate\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(estimate)","title":"<code>estimate</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Alpha and beta estimates for this trial. Should be an array of shape (n, 2) where</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(choices)","title":"<code>choices</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Choices made in this trial. Should have as many entries as there are options, with</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(outcomes)","title":"<code>outcomes</code>","text":"(<code>float</code>)           \u2013            <p>Observed outcome for this trial.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(tau_p)","title":"<code>tau_p</code>","text":"(<code>float</code>)           \u2013            <p>Update rate for outcomes equal to 1.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(tau_n)","title":"<code>tau_n</code>","text":"(<code>float</code>)           \u2013            <p>Update rate for outcomes equal to 0.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(decay)","title":"<code>decay</code>","text":"(<code>float</code>)           \u2013            <p>Decay rate.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(update)","title":"<code>update</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Whether to update the estimate. If 0, the estimate is not updated (i.e., no decay is</p>"},{"location":"reference/learning/#behavioural_modelling.learning.leaky_beta_update(increment)","title":"<code>increment</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Whether to increment the estimate. If 0, the estimate is not incremented but</p>"},{"location":"reference/learning/#behavioural_modelling.learning.multiply_beta_by_scalar","title":"multiply_beta_by_scalar","text":"<pre><code>multiply_beta_by_scalar(beta_params: ArrayLike, scalar: float) -&gt; ndarray\n</code></pre> <p>Multiply a beta distribution by a scalar.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>jnp.ndarray: New beta distribution parameters, specified as [a, b].</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef multiply_beta_by_scalar(beta_params: ArrayLike, scalar: float) -&gt; jnp.ndarray:\n    \"\"\"\n    Multiply a beta distribution by a scalar.\n\n    Args:\n        beta_params (ArrayLike): Parameters of beta distribution. Of shape (n_options, 2),\n        where the first dimension represents the number of options (each of which has its own\n        beta distribution), and the second dimension represents the alpha and beta parameters\n        of each beta distribution.\n        scalar (float): Scalar to multiply beta distribution by.\n\n    Returns:\n        jnp.ndarray: New beta distribution parameters, specified as [a, b].\n    \"\"\"\n\n    # Extract parameters\n    a = beta_params[..., 0]\n    b = beta_params[..., 1]\n\n    # Calculate mean and variance\n    mean, var = beta_mean_var(beta_params)\n\n    # Scale mean and variance\n    mean = mean * scalar\n    var = var * scalar**2\n\n    # Calculate new parameters\n    a_new = mean * ((mean * (1 - mean)) / var - 1)\n    b_new = (1 - mean) * ((mean * (1 - mean)) / var - 1)\n\n    # Return new parameters\n    return jnp.stack([a_new, b_new], axis=-1)\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.multiply_beta_by_scalar(beta_params)","title":"<code>beta_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of beta distribution. Of shape (n_options, 2),</p>"},{"location":"reference/learning/#behavioural_modelling.learning.multiply_beta_by_scalar(scalar)","title":"<code>scalar</code>","text":"(<code>float</code>)           \u2013            <p>Scalar to multiply beta distribution by.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax","title":"softmax","text":"<pre><code>softmax(value: ArrayLike, temperature: float = 1) -&gt; ArrayLike\n</code></pre> <p>Softmax function, with optional temperature parameter.</p> <p>In equation form, this is:</p> \\[ P(a) = \\frac{e^{Q(a) / \\tau}}{\\sum_{b} e^{Q(b) / \\tau}} \\] <p>Where <code>P(a)</code> is the probability of choosing action <code>a</code>, <code>Q(a)</code> is the value of action <code>a</code>, and <code>au</code> is the temperature parameter.</p> <p>Note that the value of the temperature parameter will depend on the range of the values of the Q function.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ArrayLike</code> (              <code>ArrayLike</code> )          \u2013            <p>Choice probabilities, of shape (n_trials, n_bandits)</p> </li> </ul> Source code in <code>behavioural_modelling/decision_rules.py</code> <pre><code>@jax.jit\ndef softmax(value: ArrayLike, temperature: float = 1) -&gt; ArrayLike:\n    \"\"\"\n    Softmax function, with optional temperature parameter.\n\n    In equation form, this is:\n\n    $$\n    P(a) = \\\\frac{e^{Q(a) / \\\\tau}}{\\\\sum_{b} e^{Q(b) / \\\\tau}}\n    $$\n\n    Where `P(a)` is the probability of choosing action `a`,\n    `Q(a)` is the value of action `a`, and `\\tau` is the\n    temperature parameter.\n\n    Note that the value of the temperature parameter will\n    depend on the range of the values of the Q function.\n\n    Args:\n        value (ArrayLike): Array of values to apply softmax to, of shape\n            (n_trials, n_bandits)\n        temperature (float, optional): Softmax temperature, in range [0, inf].\n            Note that this is temperature rather than inverse temperature;\n            values are divided by this value. Higher values make choices less\n            deterministic. Defaults to 1.\n\n    Returns:\n        ArrayLike: Choice probabilities, of shape (n_trials, n_bandits)\n    \"\"\"\n\n    return (jnp.exp(value / temperature)) / (\n        jnp.sum(jnp.exp(value / temperature), axis=1)[:, None]\n    )\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Array of values to apply softmax to, of shape (n_trials, n_bandits)</p>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Softmax temperature, in range [0, inf]. Note that this is temperature rather than inverse temperature; values are divided by this value. Higher values make choices less deterministic. Defaults to 1.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax_stickiness","title":"softmax_stickiness","text":"<pre><code>softmax_stickiness(value: ArrayLike, temperature: float = 1.0, stickiness: float = 0.0, prev_choice: Optional[ArrayLike] = None) -&gt; ArrayLike\n</code></pre> <p>Softmax function with choice stickiness, and optional temperature parameter.</p> <p>The standard softmax function is:</p> \\[ P(a) = \\frac{e^{Q(a) / \\tau}}{\\sum_{b} e^{Q(b) / \\tau}} \\] <p>With stickiness added:</p> \\[ P(a) = \\frac{e^{(Q(a) + \\kappa \\cdot same(a, a_{t-1}))/\\tau}} {\\sum_{b} e^{(Q(b) + \\kappa \\cdot same(b, a_{t-1}))/\\tau}} \\] <ul> <li>\\(P(a)\\) is the probability of choosing action \\(a\\)</li> <li>\\(Q(a)\\) is the value of action \\(a\\)</li> <li>\\(\\beta\\) is the temperature parameter</li> <li>\\(\\kappa\\) is the stickiness parameter</li> <li>\\(same(a, a_{t-1})\\) is 1 if \\(a\\) matches the previous choice, 0 otherwise</li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArrayLike</code> (              <code>ArrayLike</code> )          \u2013            <p>Choice probabilities, shape (n_trials, n_bandits)</p> </li> </ul> Source code in <code>behavioural_modelling/decision_rules.py</code> <pre><code>@jax.jit\ndef softmax_stickiness(\n    value: ArrayLike,\n    temperature: float = 1.0,\n    stickiness: float = 0.0,\n    prev_choice: Optional[ArrayLike] = None,\n) -&gt; ArrayLike:\n    \"\"\"\n    Softmax function with choice stickiness, and optional temperature\n    parameter.\n\n    The standard softmax function is:\n\n    $$\n    P(a) = \\\\frac{e^{Q(a) / \\\\tau}}{\\\\sum_{b} e^{Q(b) / \\\\tau}}\n    $$\n\n    With stickiness added:\n\n    $$\n    P(a) = \\\\frac{e^{(Q(a) + \\\\kappa \\\\cdot same(a, a_{t-1}))/\\\\tau}}\n    {\\\\sum_{b} e^{(Q(b) + \\\\kappa \\\\cdot same(b, a_{t-1}))/\\\\tau}}\n    $$\n\n    - $P(a)$ is the probability of choosing action $a$\n    - $Q(a)$ is the value of action $a$\n    - $\\\\beta$ is the temperature parameter\n    - $\\kappa$ is the stickiness parameter\n    - $same(a, a_{t-1})$ is 1 if $a$ matches the previous choice, 0 otherwise\n\n    Args:\n        value (ArrayLike): Array of values to apply softmax to, shape\n            `(n_trials, n_bandits)`. \n            Note that this **does not**\n            account for trial-wise dependencies, so each trial is treated\n            independently (i.e., we use precomputed choices, therefore the \n            precomputed choice on trial `t-1` can influence the choice on \n            trial `t`, but this altered choice likelihood on trial `t` will not\n            affect any subsequent trials since we rely on the precomputed\n            choices provided).\n            This can be useful to apply the same stickiness to\n            all trials, but additional code will be required to account for\n            trial-wise dependencies (i.e., the choice on trial `t-1`)\n            influencing the choice on trial `t`, and this subsequently\n            influencing trials `t+1` etc.).\n        temperature (float, optional): Softmax temperature, in range [0, inf].\n            Note that this is temperature rather than inverse temperature;\n            values are divided by this value. Higher values\n            make choices less deterministic. Defaults to 1.0.\n        stickiness (float, optional): Weight given to previous choices, range\n            (-inf, inf). Positive values increase probability of repeating\n            choices. Defaults to 0.0\n        prev_choice (ArrayLike, optional): One-hot encoded previous choices,\n            shape (n_trials, n_bandits). Defaults to None.\n\n    Returns:\n        ArrayLike: Choice probabilities, shape (n_trials, n_bandits)\n    \"\"\"\n\n    sticky_value = value + stickiness * prev_choice\n\n    return (jnp.exp(sticky_value / temperature)) / (\n        jnp.sum(jnp.exp(sticky_value / temperature), axis=1)[:, None]\n    )\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax_stickiness(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Array of values to apply softmax to, shape <code>(n_trials, n_bandits)</code>.  Note that this does not account for trial-wise dependencies, so each trial is treated independently (i.e., we use precomputed choices, therefore the  precomputed choice on trial <code>t-1</code> can influence the choice on  trial <code>t</code>, but this altered choice likelihood on trial <code>t</code> will not affect any subsequent trials since we rely on the precomputed choices provided). This can be useful to apply the same stickiness to all trials, but additional code will be required to account for trial-wise dependencies (i.e., the choice on trial <code>t-1</code>) influencing the choice on trial <code>t</code>, and this subsequently influencing trials <code>t+1</code> etc.).</p>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax_stickiness(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Softmax temperature, in range [0, inf]. Note that this is temperature rather than inverse temperature; values are divided by this value. Higher values make choices less deterministic. Defaults to 1.0.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax_stickiness(stickiness)","title":"<code>stickiness</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Weight given to previous choices, range (-inf, inf). Positive values increase probability of repeating choices. Defaults to 0.0</p>"},{"location":"reference/learning/#behavioural_modelling.learning.softmax_stickiness(prev_choice)","title":"<code>prev_choice</code>","text":"(<code>ArrayLike</code>, default:                   <code>None</code> )           \u2013            <p>One-hot encoded previous choices, shape (n_trials, n_bandits). Defaults to None.</p>"},{"location":"reference/learning/#behavioural_modelling.learning.sum_betas","title":"sum_betas","text":"<pre><code>sum_betas(beta1_params: ArrayLike, beta2_params: ArrayLike) -&gt; ndarray\n</code></pre> <p>Sum two beta distributions. This uses an approximation described in the following paper:</p> <p>Pham, T.G., Turkkan, N., 1994. Reliability of a standby system with beta-distributed component lives. IEEE Transactions on Reliability 43, 71\u201375. https://doi.org/10.1109/24.285114</p> <p>Where the first two moments of the summed distribution are calculated as follows:</p> \\[ \\mu = \\mu_1 + \\mu_2 \\] \\[ \\sigma^2 = \\sigma_1^2 + \\sigma_2^2 \\] <p>We then calculate the parameters of the new beta distribution using the following equations:</p> \\[ \\alpha = \\mu \\left( \\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1 \\right) \\] \\[ \\beta = (1 - \\mu) \\left( \\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1 \\right) \\] <p>This function assumes that the means of the two beta distributions sum to &lt;=1. If this is not the case, the output will be invalid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>jnp.ndarray: New beta distribution parameters.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef sum_betas(beta1_params: ArrayLike, beta2_params: ArrayLike) -&gt; jnp.ndarray:\n    \"\"\"\n    Sum two beta distributions. This uses an approximation described in the following paper:\n\n    Pham, T.G., Turkkan, N., 1994. Reliability of a standby system with beta-distributed component lives.\n    IEEE Transactions on Reliability 43, 71\u201375. https://doi.org/10.1109/24.285114\n\n    Where the first two moments of the summed distribution are calculated as follows:\n\n    $$\n    \\\\mu = \\\\mu_1 + \\\\mu_2\n    $$\n\n    $$\n    \\\\sigma^2 = \\\\sigma_1^2 + \\\\sigma_2^2\n    $$\n\n    We then calculate the parameters of the new beta distribution using the following equations:\n\n    $$\n    \\\\alpha = \\\\mu \\\\left( \\\\frac{\\\\mu (1 - \\\\mu)}{\\\\sigma^2} - 1 \\\\right)\n    $$\n\n    $$\n    \\\\beta = (1 - \\\\mu) \\\\left( \\\\frac{\\\\mu (1 - \\\\mu)}{\\\\sigma^2} - 1 \\\\right)\n    $$\n\n    This function assumes that the means of the two beta distributions sum to &lt;=1. If this is not the case,\n    the output will be invalid.\n\n    Args:\n        beta1_params (ArrayLike): Parameters of the first beta distribution. Of shape (n_options, 2),\n        where the first dimension represents the number of options (each of which has its own\n        beta distribution), and the sec\n        beta2_params (ArrayLike): Parameters of second beta distribution.\n\n    Returns:\n        jnp.ndarray: New beta distribution parameters.\n    \"\"\"\n\n    # Extract parameters\n    a1 = beta1_params[..., 0]\n    b1 = beta1_params[..., 1]\n    a2 = beta2_params[..., 0]\n    b2 = beta2_params[..., 1]\n\n    # Calculate means and variances\n    mean1 = a1 / (a1 + b1)\n    var1 = (a1 * b1) / ((a1 + b1) ** 2 * (a1 + b1 + 1))\n    mean2 = a2 / (a2 + b2)\n    var2 = (a2 * b2) / ((a2 + b2) ** 2 * (a2 + b2 + 1))\n\n    # Sum means and variances\n    mean_new = mean1 + mean2\n    var_new = var1 + var2\n\n    # Calculate new parameters\n    a_new = mean_new * ((mean_new * (1 - mean_new)) / var_new - 1)\n    b_new = (1 - mean_new) * ((mean_new * (1 - mean_new)) / var_new - 1)\n\n    # Return new parameters\n    return jnp.stack([a_new, b_new], axis=-1)\n</code></pre>"},{"location":"reference/learning/#behavioural_modelling.learning.sum_betas(beta1_params)","title":"<code>beta1_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of the first beta distribution. Of shape (n_options, 2),</p>"},{"location":"reference/learning/#behavioural_modelling.learning.sum_betas(beta2_params)","title":"<code>beta2_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of second beta distribution.</p>"},{"location":"reference/planning/","title":"Planning models","text":"<p>This module contains planning models.</p>"},{"location":"reference/planning/#behavioural_modelling.planning","title":"planning","text":"<p>Modules:</p> <ul> <li> <code>dynamic_programming</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>get_state_action_values</code>             \u2013              <p>Calculates the value of each action for a given state. Used within the main</p> </li> <li> <code>state_value_iterator</code>             \u2013              <p>Core value iteration function - calculates value function for the MDP and</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>solve_value_iteration</code>               (<code>Tuple[ndarray, ndarray]</code>)           \u2013            <p>Solves an MDP using value iteration given a reward function.</p> </li> </ul>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration","title":"solve_value_iteration  <code>module-attribute</code>","text":"<pre><code>solve_value_iteration: Tuple[ndarray, ndarray] = jit(solve_value_iteration, static_argnums=(0, 1))\n</code></pre> <p>Solves an MDP using value iteration given a reward function.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ndarray, ndarray]</code>           \u2013            <p>Tuple[jnp.ndarray, jnp.ndarray]: Final value function and action values (Q-values)</p> </li> </ul>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration(n_states)","title":"<code>n_states</code>","text":"(<code>int</code>)           \u2013            <p>Number of states</p>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>Number of actions</p>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration(reward_function)","title":"<code>reward_function</code>","text":"(<code>ndarray</code>)           \u2013            <p>Reward function (i.e., reward at each state)</p>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration(max_iter)","title":"<code>max_iter</code>","text":"(<code>int</code>)           \u2013            <p>Maximum number of iterations</p>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration(discount)","title":"<code>discount</code>","text":"(<code>float</code>)           \u2013            <p>Discount factor</p>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration(sas)","title":"<code>sas</code>","text":"(<code>ndarray</code>)           \u2013            <p>State-action-state transition probabilities</p>"},{"location":"reference/planning/#behavioural_modelling.planning.solve_value_iteration(tol)","title":"<code>tol</code>","text":"(<code>float</code>)           \u2013            <p>Tolerance for convergence</p>"},{"location":"reference/planning/#behavioural_modelling.planning.get_state_action_values","title":"get_state_action_values","text":"<pre><code>get_state_action_values(s: int, n_actions: int, sas: ndarray, reward: ndarray, discount: float, values: ndarray) -&gt; ndarray\n</code></pre> <p>Calculates the value of each action for a given state. Used within the main value iteration loop.</p> <p>Reward is typically conceived of as resulting from taking action A in state S. Here, we for the sake of simplicity, we assume that the reward results from visiting state S' - that is, taking action A in state S isn't rewarding in itself, but the reward received is dependent on the reward present in state S'.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Estimated value of each state</p> </li> </ul> Source code in <code>behavioural_modelling/planning/dynamic_programming.py</code> <pre><code>def get_state_action_values(\n    s: int,\n    n_actions: int,\n    sas: jnp.ndarray,\n    reward: jnp.ndarray,\n    discount: float,\n    values: jnp.ndarray,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Calculates the value of each action for a given state. Used within the main\n    value iteration loop.\n\n    Reward is typically conceived of as resulting from taking action A in state\n    S. Here, we for the sake of simplicity, we assume that the reward results\n    from visiting state S' - that is, taking action A in state S isn't\n    rewarding in itself, but the reward received is dependent on the reward\n    present in state S'.\n\n    Args:\n        s (int): State ID\n        n_actions (int): Number of possible actions\n        sas (np.ndarray): State, action, state transition function\n        reward (np.ndarray): Reward available at each state\n        discount (float): Discount factor\n        values (np.ndarray): Current estimate of value function\n\n\n    Returns:\n        np.ndarray: Estimated value of each state\n    \"\"\"\n\n    def action_update(s, a, sas, reward, discount, values):\n        p_sprime = sas[s, a, :]\n        return jnp.dot(p_sprime, reward + discount * values)\n\n    action_values = jax.vmap(\n        action_update, in_axes=(None, 0, None, None, None, None)\n    )(s, jnp.arange(n_actions, dtype=int), sas, reward, discount, values)\n\n    return action_values\n</code></pre>"},{"location":"reference/planning/#behavioural_modelling.planning.get_state_action_values(s)","title":"<code>s</code>","text":"(<code>int</code>)           \u2013            <p>State ID</p>"},{"location":"reference/planning/#behavioural_modelling.planning.get_state_action_values(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>Number of possible actions</p>"},{"location":"reference/planning/#behavioural_modelling.planning.get_state_action_values(sas)","title":"<code>sas</code>","text":"(<code>ndarray</code>)           \u2013            <p>State, action, state transition function</p>"},{"location":"reference/planning/#behavioural_modelling.planning.get_state_action_values(reward)","title":"<code>reward</code>","text":"(<code>ndarray</code>)           \u2013            <p>Reward available at each state</p>"},{"location":"reference/planning/#behavioural_modelling.planning.get_state_action_values(discount)","title":"<code>discount</code>","text":"(<code>float</code>)           \u2013            <p>Discount factor</p>"},{"location":"reference/planning/#behavioural_modelling.planning.get_state_action_values(values)","title":"<code>values</code>","text":"(<code>ndarray</code>)           \u2013            <p>Current estimate of value function</p>"},{"location":"reference/planning/#behavioural_modelling.planning.state_value_iterator","title":"state_value_iterator","text":"<pre><code>state_value_iterator(values: ndarray, reward: ndarray, discount: float, sas: ndarray, soft: bool = False) -&gt; Tuple[ndarray, float, ndarray]\n</code></pre> <p>Core value iteration function - calculates value function for the MDP and returns q-values for each action in each state.</p> <p>This function just runs one iteration of the value iteration algorithm.</p> <p>\"Soft\" value iteration can optionally be performed. This essentially involves taking the softmax of action values rather than the max, and is useful for inverse reinforcement learning (see Bloem &amp; Bambos, 2014).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ndarray, float, ndarray]</code>           \u2013            <p>Tuple[np.ndarray, float, np.ndarray]: Returns new estimate of the value function, new delta, and new q_values</p> </li> </ul> Source code in <code>behavioural_modelling/planning/dynamic_programming.py</code> <pre><code>def state_value_iterator(\n    values: jnp.ndarray,\n    reward: jnp.ndarray,\n    discount: float,\n    sas: jnp.ndarray,\n    soft: bool = False,\n) -&gt; Tuple[jnp.ndarray, float, jnp.ndarray]:\n    \"\"\"\n    Core value iteration function - calculates value function for the MDP and\n    returns q-values for each action in each state.\n\n    This function just runs one iteration of the value iteration algorithm.\n\n    \"Soft\" value iteration can optionally be performed. This essentially\n    involves taking the softmax of action values rather than the max, and is\n    useful for inverse reinforcement learning (see Bloem &amp; Bambos, 2014).\n\n    Args:\n        values (np.ndarray): Current estimate of the value function\n        reward (np.ndarray): Reward at each state (i.e. features x reward\n            function)\n        discount (float): Discount factor\n        sas (np.ndarray): State, action, state transition function\n        soft (bool, optional): If True, this implements \"soft\" value iteration\n            rather than standard value iteration. Defaults to False.\n\n    Returns:\n        Tuple[np.ndarray, float, np.ndarray]: Returns new estimate of the value\n            function, new delta, and new q_values\n    \"\"\"\n    n_states, n_actions = sas.shape[:2]\n    q_values = jnp.zeros((n_states, n_actions))\n\n    def scan_fn(values_delta, s):\n\n        values, delta = values_delta\n\n        v = values[s]  # Current value estimate for state `s`\n        action_values = get_state_action_values(\n            s, n_actions, sas, reward, discount, values\n        )\n\n        if not soft:\n            new_value = jnp.max(action_values)\n        else:\n            new_value = jnp.log(jnp.sum(jnp.exp(action_values)) + 1e-200)\n\n        # Update Q-values for state `s`\n        q_values_s = action_values\n\n        # Update delta\n        delta = jnp.abs(new_value - v)\n\n        # Update value for state `s`\n        values = values.at[s].set(new_value)\n\n        return (values, delta), q_values_s\n\n    # Perform the sequential scan\n    (new_values, final_delta), all_q_values = jax.lax.scan(\n        scan_fn, (values, 0), jnp.arange(n_states)\n    )\n\n    # Combine all Q-values into a single array\n    q_values = q_values.at[:, :].set(all_q_values)\n\n    return new_values, final_delta, q_values\n</code></pre>"},{"location":"reference/planning/#behavioural_modelling.planning.state_value_iterator(values)","title":"<code>values</code>","text":"(<code>ndarray</code>)           \u2013            <p>Current estimate of the value function</p>"},{"location":"reference/planning/#behavioural_modelling.planning.state_value_iterator(reward)","title":"<code>reward</code>","text":"(<code>ndarray</code>)           \u2013            <p>Reward at each state (i.e. features x reward function)</p>"},{"location":"reference/planning/#behavioural_modelling.planning.state_value_iterator(discount)","title":"<code>discount</code>","text":"(<code>float</code>)           \u2013            <p>Discount factor</p>"},{"location":"reference/planning/#behavioural_modelling.planning.state_value_iterator(sas)","title":"<code>sas</code>","text":"(<code>ndarray</code>)           \u2013            <p>State, action, state transition function</p>"},{"location":"reference/planning/#behavioural_modelling.planning.state_value_iterator(soft)","title":"<code>soft</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, this implements \"soft\" value iteration rather than standard value iteration. Defaults to False.</p>"},{"location":"reference/utils/","title":"Utilities","text":""},{"location":"reference/utils/#behavioural_modelling.utils","title":"utils","text":"<p>Functions:</p> <ul> <li> <code>choice_from_action_p</code>             \u2013              <p>Choose an action from a set of action probabilities. Can take probabilities</p> </li> <li> <code>choice_from_action_p_single</code>             \u2013              <p>Choose an action from a set of action probabilities for a single choice.</p> </li> </ul>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p","title":"choice_from_action_p","text":"<pre><code>choice_from_action_p(key: PRNGKey, probs: ArrayLike, lapse: float = 0.0) -&gt; int\n</code></pre> <p>Choose an action from a set of action probabilities. Can take probabilities in the form of an n-dimensional array, where the last dimension is the number of actions.</p> <p>Noise is added to the choice, with probability <code>lapse</code>. This means that on \"lapse\" trials, the subject will choose an action uniformly at random.</p> <p>Parameters:</p> <p>Returns:     int: Chosen action</p> Source code in <code>behavioural_modelling/utils.py</code> <pre><code>@jax.jit\ndef choice_from_action_p(key: jax.random.PRNGKey, probs: ArrayLike, lapse: float = 0.0) -&gt; int:\n    \"\"\"\n    Choose an action from a set of action probabilities. Can take probabilities\n    in the form of an n-dimensional array, where the last dimension is the\n    number of actions.\n\n    Noise is added to the choice, with probability `lapse`. This means that\n    on \"lapse\" trials, the subject will choose an action uniformly at random.\n\n    Args:\n        key (int): Jax random key\n        probs (np.ndarray): N-dimension array of action probabilities, of shape (..., n_actions)\n        lapse (float, optional): Probability of lapse. Defaults to 0.0.\n    Returns:\n        int: Chosen action\n    \"\"\"\n\n    # Reshape probs\n    probs_reshaped = probs.reshape((-1, probs.shape[-1]))\n\n    # Split keys so that we have one for each index in the first\n    # dimension of probs\n    keys = jax.random.split(key, probs_reshaped.shape[0])\n\n    # Get choices\n    choices = choice_func_vmap(keys, probs_reshaped, lapse)\n\n    # Reshape choices\n    choices = choices.reshape(probs.shape[:-1])\n\n    return choices\n</code></pre>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p(key)","title":"<code>key</code>","text":"(<code>int</code>)           \u2013            <p>Jax random key</p>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p(probs)","title":"<code>probs</code>","text":"(<code>ndarray</code>)           \u2013            <p>N-dimension array of action probabilities, of shape (..., n_actions)</p>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p(lapse)","title":"<code>lapse</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Probability of lapse. Defaults to 0.0.</p>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p_single","title":"choice_from_action_p_single","text":"<pre><code>choice_from_action_p_single(key: PRNGKey, probs: ArrayLike, lapse: float = 0.0) -&gt; int\n</code></pre> <p>Choose an action from a set of action probabilities for a single choice.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>Chosen action</p> </li> </ul> Source code in <code>behavioural_modelling/utils.py</code> <pre><code>@jax.jit\ndef choice_from_action_p_single(\n    key: jax.random.PRNGKey, probs: ArrayLike, lapse: float = 0.0\n) -&gt; int:\n    \"\"\"\n    Choose an action from a set of action probabilities for a single choice.\n\n    Args:\n        key (jax.random.PRNGKey): Jax random key\n        probs (ArrayLike): 1D array of action probabilities, of shape (n_actions)\n        lapse (float, optional): Lapse parameter. On lapse trials, a random action is selected. Defaults to 0.0.\n\n    Returns:\n        int: Chosen action\n    \"\"\"\n\n    # Get number of possible actions\n    n_actions = len(probs)\n\n    # Deal with zero values etc\n    probs = probs + 1e-6 / jnp.sum(probs)\n\n    # Add noise\n    noise = jax.random.uniform(key) &lt; lapse\n\n    # Choose action\n    choice = (1 - noise) * jax.random.choice(\n        key, jnp.arange(n_actions, dtype=int), p=probs\n    ) + noise * jax.random.randint(key, shape=(), minval=0, maxval=n_actions)\n\n    return choice\n</code></pre>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p_single(key)","title":"<code>key</code>","text":"(<code>PRNGKey</code>)           \u2013            <p>Jax random key</p>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p_single(probs)","title":"<code>probs</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>1D array of action probabilities, of shape (n_actions)</p>"},{"location":"reference/utils/#behavioural_modelling.utils.choice_from_action_p_single(lapse)","title":"<code>lapse</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Lapse parameter. On lapse trials, a random action is selected. Defaults to 0.0.</p>"},{"location":"reference/learning/beta_models/","title":"Beta Models","text":""},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models","title":"beta_models","text":"<p>Functions:</p> <ul> <li> <code>average_betas</code>             \u2013              <p>Average two beta distributions, weighted by W.</p> </li> <li> <code>beta_mean_var</code>             \u2013              <p>Calculate mean and variance of a beta distribution.</p> </li> <li> <code>generalised_beta_mean_var</code>             \u2013              <p>Calculate mean and variance of a generalised beta distribution.</p> </li> <li> <code>leaky_beta_update</code>             \u2013              <p>Update estimates using the (asymmetric) leaky beta model. </p> </li> <li> <code>multiply_beta_by_scalar</code>             \u2013              <p>Multiply a beta distribution by a scalar.</p> </li> <li> <code>sum_betas</code>             \u2013              <p>Sum two beta distributions. This uses an approximation described in the following paper:</p> </li> </ul>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.average_betas","title":"average_betas","text":"<pre><code>average_betas(beta1_params: ArrayLike, beta2_params: ArrayLike, W1: float = 0.5, W2: float = 0.5) -&gt; ndarray\n</code></pre> <p>Average two beta distributions, weighted by W.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>jnp.ndarray: New beta distribution parameters.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef average_betas(\n    beta1_params: ArrayLike,\n    beta2_params: ArrayLike,\n    W1: float = 0.5,\n    W2: float = 0.5,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Average two beta distributions, weighted by W.\n\n    Args:\n        beta1_params (ArrayLike): Parameters of first beta distribution.\n        beta2_params (ArrayLike): Parameters of second beta distribution.\n\n    Returns:\n        jnp.ndarray: New beta distribution parameters.\n    \"\"\"\n\n    # Extract parameters\n    a1 = beta1_params[..., 0]\n    b1 = beta1_params[..., 1]\n    a2 = beta2_params[..., 0]\n    b2 = beta2_params[..., 1]\n\n    # Calculate average\n    a_new = (W1 * a1) + (W2 * a2)\n    b_new = (W1 * b1) + (W2 * b2)\n\n    # Return new parameters\n    return jnp.stack([a_new, b_new], axis=-1)\n</code></pre>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.average_betas(beta1_params)","title":"<code>beta1_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of first beta distribution.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.average_betas(beta2_params)","title":"<code>beta2_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of second beta distribution.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.beta_mean_var","title":"beta_mean_var","text":"<pre><code>beta_mean_var(beta_params: ArrayLike) -&gt; Tuple[ArrayLike, ArrayLike]\n</code></pre> <p>Calculate mean and variance of a beta distribution.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ArrayLike, ArrayLike]</code>           \u2013            <p>tuple[ArrayLike, ArrayLike]: Mean and variance of the beta distribution.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef beta_mean_var(beta_params: ArrayLike) -&gt; Tuple[ArrayLike, ArrayLike]:\n    \"\"\"\n    Calculate mean and variance of a beta distribution.\n\n    Args:\n        beta_params (ArrayLike): Parameters of the beta distribution. Of shape (n_options, 2),\n        where the first dimension represents the number of options (each of which has its own\n        beta distribution), and the second dimension represents the alpha and beta parameters\n        of each beta distribution.\n\n    Returns:\n        tuple[ArrayLike, ArrayLike]: Mean and variance of the beta distribution.\n    \"\"\"\n    a, b = beta_params[..., 0], beta_params[..., 1]\n    mean = a / (a + b)\n    var = (a * b) / ((a + b) ** 2 * (a + b + 1))\n    return mean, var\n</code></pre>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.beta_mean_var(beta_params)","title":"<code>beta_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of the beta distribution. Of shape (n_options, 2),</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.generalised_beta_mean_var","title":"generalised_beta_mean_var","text":"<pre><code>generalised_beta_mean_var(alpha: float, beta: float, a: float, b: float) -&gt; Tuple[float, float]\n</code></pre> <p>Calculate mean and variance of a generalised beta distribution.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[float, float]</code>           \u2013            <p>tuple[float, float]: Mean and variance of the beta distribution.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef generalised_beta_mean_var(\n    alpha: float, beta: float, a: float, b: float\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Calculate mean and variance of a generalised beta distribution.\n\n    Args:\n        alpha (float): Alpha parameter of the beta distribution.\n        beta (float): Beta parameter of the beta distribution.\n        a (float): Lower bound of the beta distribution.\n        b (float): Upper bound of the beta distribution.\n\n    Returns:\n        tuple[float, float]: Mean and variance of the beta distribution.\n    \"\"\"\n    mean = ((b - a) * alpha) / (alpha + beta) + a\n    var = (alpha * beta * (b - a) ** 2) / ((alpha + beta) ** 2 * (alpha + beta + 1))\n    return mean, var\n</code></pre>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.generalised_beta_mean_var(alpha)","title":"<code>alpha</code>","text":"(<code>float</code>)           \u2013            <p>Alpha parameter of the beta distribution.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.generalised_beta_mean_var(beta)","title":"<code>beta</code>","text":"(<code>float</code>)           \u2013            <p>Beta parameter of the beta distribution.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.generalised_beta_mean_var(a)","title":"<code>a</code>","text":"(<code>float</code>)           \u2013            <p>Lower bound of the beta distribution.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.generalised_beta_mean_var(b)","title":"<code>b</code>","text":"(<code>float</code>)           \u2013            <p>Upper bound of the beta distribution.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update","title":"leaky_beta_update","text":"<pre><code>leaky_beta_update(estimate: ArrayLike, choices: ArrayLike, outcome: float, tau_p: float, tau_n: float, decay: float, update: int = 1, increment: int = 1) -&gt; ndarray\n</code></pre> <p>Update estimates using the (asymmetric) leaky beta model. </p> <p>This models represents the probability of the outcome associated with each option (e.g., bandits in a bandit task) as a beta distribution.</p> <p>Values are updated according to the following equations:</p> \\[ A_i^{t+1} = \\lambda \\cdot A_i^{t} + outcome_t \\cdot \\tau^{+} \\] \\[ B_i^{t+1} = \\lambda \\cdot B_i^{t} + (1-outcome_t) \\cdot \\tau^{-} \\] <p>This function also allows for updating to be turned off (i.e., the estimate is not updated at all) and for incrementing to be turned off (i.e., decay is applied, but the outcome is not registered).</p> <p>Only chosen options incremented, but all options decay.</p> <p>Parameters:</p> <p>Returns:     jnp.ndarray: Updated value estimates for this trial, with one entry per bandit.</p> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef leaky_beta_update(\n    estimate: ArrayLike,\n    choices: ArrayLike,\n    outcome: float,\n    tau_p: float,\n    tau_n: float,\n    decay: float,\n    update: int = 1,\n    increment: int = 1,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Update estimates using the (asymmetric) leaky beta model. \n\n    This models represents the probability of the outcome associated with each option (e.g., bandits in a bandit task)\n    as a beta distribution.\n\n    Values are updated according to the following equations:\n\n    $$\n    A_i^{t+1} = \\\\lambda \\\\cdot A_i^{t} + outcome_t \\\\cdot \\\\tau^{+}\n    $$\n\n    $$\n    B_i^{t+1} = \\\\lambda \\\\cdot B_i^{t} + (1-outcome_t) \\\\cdot \\\\tau^{-}\n    $$\n\n    This function also allows for updating to be turned off (i.e., the estimate is not updated at all) and for incrementing\n    to be turned off (i.e., decay is applied, but the outcome is not registered).\n\n    Only chosen options incremented, but all options decay.\n\n    Args:\n        estimate (ArrayLike): Alpha and beta estimates for this trial. Should be an array of shape (n, 2) where\n        the first dimension represents the alpha and beta parameters of the beta distribution and the second\n        dimension represents the number of option.\n        choices (ArrayLike): Choices made in this trial. Should have as many entries as there are options, with\n        zeros for non-chosen options and ones for chosen options (i.e., one-hot encoded).\n        outcomes (float): Observed outcome for this trial.\n        tau_p (float): Update rate for outcomes equal to 1.\n        tau_n (float): Update rate for outcomes equal to 0.\n        decay (float): Decay rate.\n        update (int, optional): Whether to update the estimate. If 0, the estimate is not updated (i.e., no decay is\n        applied, and the outcome of the trial does not affect the outcome). Defaults to 1.\n        increment (int, optional): Whether to increment the estimate. If 0, the estimate is not incremented but\n        decay is applied. Defaults to 1.\n    Returns:\n        jnp.ndarray: Updated value estimates for this trial, with one entry per bandit.\n    \"\"\"\n\n    # For each parameter, we apply the decay to (previous value - 1) so that we are in effect\n    # treating 1 as the baseline value. This is helpful becuase values of &lt; 1 can produce\n    # strange-looking distributions (e.g., with joint peaks at 0 and 1). Keeping values\n    # &gt; 1 ensures that the baseline distribution (ignoring any evidence we've observed)\n    # is a flat distribution between 0 and 1. This also generally aids parameter recovery.\n\n    # Make sure any outcomes &gt; 1 are set to 1\n    outcome = jnp.array(outcome &gt; 0, int)\n\n    # Update alpha\n    update_1 = (\n        1 + (decay * (estimate[:, 0] - 1)) + (tau_p * (choices * outcome) * increment)\n    )\n    estimate = estimate.at[:, 0].set(\n        (update * update_1) + ((1 - update) * estimate[:, 0])\n    )\n\n    # Update beta\n    update_2 = (\n        1\n        + (decay * (estimate[:, 1] - 1))\n        + (tau_n * (choices * (1 - outcome)) * increment)\n    )\n    estimate = estimate.at[:, 1].set(\n        (update * update_2) + ((1 - update) * estimate[:, 1])\n    )\n\n    return estimate\n</code></pre>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(estimate)","title":"<code>estimate</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Alpha and beta estimates for this trial. Should be an array of shape (n, 2) where</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(choices)","title":"<code>choices</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Choices made in this trial. Should have as many entries as there are options, with</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(outcomes)","title":"<code>outcomes</code>","text":"(<code>float</code>)           \u2013            <p>Observed outcome for this trial.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(tau_p)","title":"<code>tau_p</code>","text":"(<code>float</code>)           \u2013            <p>Update rate for outcomes equal to 1.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(tau_n)","title":"<code>tau_n</code>","text":"(<code>float</code>)           \u2013            <p>Update rate for outcomes equal to 0.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(decay)","title":"<code>decay</code>","text":"(<code>float</code>)           \u2013            <p>Decay rate.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(update)","title":"<code>update</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Whether to update the estimate. If 0, the estimate is not updated (i.e., no decay is</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.leaky_beta_update(increment)","title":"<code>increment</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Whether to increment the estimate. If 0, the estimate is not incremented but</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.multiply_beta_by_scalar","title":"multiply_beta_by_scalar","text":"<pre><code>multiply_beta_by_scalar(beta_params: ArrayLike, scalar: float) -&gt; ndarray\n</code></pre> <p>Multiply a beta distribution by a scalar.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>jnp.ndarray: New beta distribution parameters, specified as [a, b].</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef multiply_beta_by_scalar(beta_params: ArrayLike, scalar: float) -&gt; jnp.ndarray:\n    \"\"\"\n    Multiply a beta distribution by a scalar.\n\n    Args:\n        beta_params (ArrayLike): Parameters of beta distribution. Of shape (n_options, 2),\n        where the first dimension represents the number of options (each of which has its own\n        beta distribution), and the second dimension represents the alpha and beta parameters\n        of each beta distribution.\n        scalar (float): Scalar to multiply beta distribution by.\n\n    Returns:\n        jnp.ndarray: New beta distribution parameters, specified as [a, b].\n    \"\"\"\n\n    # Extract parameters\n    a = beta_params[..., 0]\n    b = beta_params[..., 1]\n\n    # Calculate mean and variance\n    mean, var = beta_mean_var(beta_params)\n\n    # Scale mean and variance\n    mean = mean * scalar\n    var = var * scalar**2\n\n    # Calculate new parameters\n    a_new = mean * ((mean * (1 - mean)) / var - 1)\n    b_new = (1 - mean) * ((mean * (1 - mean)) / var - 1)\n\n    # Return new parameters\n    return jnp.stack([a_new, b_new], axis=-1)\n</code></pre>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.multiply_beta_by_scalar(beta_params)","title":"<code>beta_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of beta distribution. Of shape (n_options, 2),</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.multiply_beta_by_scalar(scalar)","title":"<code>scalar</code>","text":"(<code>float</code>)           \u2013            <p>Scalar to multiply beta distribution by.</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.sum_betas","title":"sum_betas","text":"<pre><code>sum_betas(beta1_params: ArrayLike, beta2_params: ArrayLike) -&gt; ndarray\n</code></pre> <p>Sum two beta distributions. This uses an approximation described in the following paper:</p> <p>Pham, T.G., Turkkan, N., 1994. Reliability of a standby system with beta-distributed component lives. IEEE Transactions on Reliability 43, 71\u201375. https://doi.org/10.1109/24.285114</p> <p>Where the first two moments of the summed distribution are calculated as follows:</p> \\[ \\mu = \\mu_1 + \\mu_2 \\] \\[ \\sigma^2 = \\sigma_1^2 + \\sigma_2^2 \\] <p>We then calculate the parameters of the new beta distribution using the following equations:</p> \\[ \\alpha = \\mu \\left( \\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1 \\right) \\] \\[ \\beta = (1 - \\mu) \\left( \\frac{\\mu (1 - \\mu)}{\\sigma^2} - 1 \\right) \\] <p>This function assumes that the means of the two beta distributions sum to &lt;=1. If this is not the case, the output will be invalid.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>jnp.ndarray: New beta distribution parameters.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/beta_models.py</code> <pre><code>@jax.jit\ndef sum_betas(beta1_params: ArrayLike, beta2_params: ArrayLike) -&gt; jnp.ndarray:\n    \"\"\"\n    Sum two beta distributions. This uses an approximation described in the following paper:\n\n    Pham, T.G., Turkkan, N., 1994. Reliability of a standby system with beta-distributed component lives.\n    IEEE Transactions on Reliability 43, 71\u201375. https://doi.org/10.1109/24.285114\n\n    Where the first two moments of the summed distribution are calculated as follows:\n\n    $$\n    \\\\mu = \\\\mu_1 + \\\\mu_2\n    $$\n\n    $$\n    \\\\sigma^2 = \\\\sigma_1^2 + \\\\sigma_2^2\n    $$\n\n    We then calculate the parameters of the new beta distribution using the following equations:\n\n    $$\n    \\\\alpha = \\\\mu \\\\left( \\\\frac{\\\\mu (1 - \\\\mu)}{\\\\sigma^2} - 1 \\\\right)\n    $$\n\n    $$\n    \\\\beta = (1 - \\\\mu) \\\\left( \\\\frac{\\\\mu (1 - \\\\mu)}{\\\\sigma^2} - 1 \\\\right)\n    $$\n\n    This function assumes that the means of the two beta distributions sum to &lt;=1. If this is not the case,\n    the output will be invalid.\n\n    Args:\n        beta1_params (ArrayLike): Parameters of the first beta distribution. Of shape (n_options, 2),\n        where the first dimension represents the number of options (each of which has its own\n        beta distribution), and the sec\n        beta2_params (ArrayLike): Parameters of second beta distribution.\n\n    Returns:\n        jnp.ndarray: New beta distribution parameters.\n    \"\"\"\n\n    # Extract parameters\n    a1 = beta1_params[..., 0]\n    b1 = beta1_params[..., 1]\n    a2 = beta2_params[..., 0]\n    b2 = beta2_params[..., 1]\n\n    # Calculate means and variances\n    mean1 = a1 / (a1 + b1)\n    var1 = (a1 * b1) / ((a1 + b1) ** 2 * (a1 + b1 + 1))\n    mean2 = a2 / (a2 + b2)\n    var2 = (a2 * b2) / ((a2 + b2) ** 2 * (a2 + b2 + 1))\n\n    # Sum means and variances\n    mean_new = mean1 + mean2\n    var_new = var1 + var2\n\n    # Calculate new parameters\n    a_new = mean_new * ((mean_new * (1 - mean_new)) / var_new - 1)\n    b_new = (1 - mean_new) * ((mean_new * (1 - mean_new)) / var_new - 1)\n\n    # Return new parameters\n    return jnp.stack([a_new, b_new], axis=-1)\n</code></pre>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.sum_betas(beta1_params)","title":"<code>beta1_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of the first beta distribution. Of shape (n_options, 2),</p>"},{"location":"reference/learning/beta_models/#behavioural_modelling.learning.beta_models.sum_betas(beta2_params)","title":"<code>beta2_params</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>Parameters of second beta distribution.</p>"},{"location":"reference/learning/rescorla_wagner/","title":"Rescorla-Wagner Models","text":"<p>This module incorporates includes various implementations of the Rescorla-Wagner model.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner","title":"rescorla_wagner","text":"<p>Functions:</p> <ul> <li> <code>asymmetric_volatile_rescorla_wagner_update</code>             \u2013              <p>Updates the estimated value of a state or action using a variant</p> </li> <li> <code>complement_counterfactual</code>             \u2013              <p>Counterfactual function that sets the value of unchosen actions to the</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>asymmetric_rescorla_wagner_update</code>               (<code>Tuple[ArrayLike, ArrayLike]</code>)           \u2013            <p>Updates the estimated value of a state or action using the Asymmetric</p> </li> <li> <code>asymmetric_rescorla_wagner_update_choice</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using the asymmetric Rescorla-Wagner</p> </li> <li> <code>asymmetric_rescorla_wagner_update_choice_sticky</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using the asymmetric Rescorla-Wagner</p> </li> <li> <code>asymmetric_volatile_dynamic_rescorla_wagner_update_choice</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using a variant of the Rescorla-Wagner</p> </li> <li> <code>asymmetric_volatile_rescorla_wagner_single_value_update_choice</code>               (<code>Array</code>)           \u2013            <p>Updates the value estimate using the asymmetric volatile dynamic</p> </li> </ul>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update","title":"asymmetric_rescorla_wagner_update  <code>module-attribute</code>","text":"<pre><code>asymmetric_rescorla_wagner_update: Tuple[ArrayLike, ArrayLike] = jit(asymmetric_rescorla_wagner_update, static_argnames='counterfactual_value')\n</code></pre> <p>Updates the estimated value of a state or action using the Asymmetric Rescorla-Wagner learning rule.</p> <p>The function calculates the prediction error as the difference between the actual outcome and the current estimated value. It then updates the estimated value based on the prediction error and the learning rate, which is determined by whether the prediction error is positive or negative.</p> <p>Value estimates are only updated for chosen actions. For unchosen actions, the prediction error is set to 0.</p> <p>Counterfactual updating can be used to set the value of unchosen actions according to a function of the value of chosen actions. This can be useful in cases where the value of unchosen actions should be set to a specific value, such as the negative of the value of chosen actions. By default this function sets the value of unchosen actions to the complement of the value of chosen actions:</p> <pre><code>counterfactual_value: callable = lambda reward, chosen: jnp.where(\n    chosen == 1, \n    0.0, \n    1.0 - jnp.sum(reward * jnp.asarray(chosen == 1, dtype=reward.dtype))\n)\n</code></pre> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ArrayLike, ArrayLike]</code>           \u2013            <p>Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]: The updated value and the prediction error.</p> </li> </ul>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current estimated value of a state or action.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update(alpha_p)","title":"<code>alpha_p</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The learning rate used when the prediction error is positive.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update(alpha_n)","title":"<code>alpha_n</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The learning rate used when the prediction error is negative.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update(counterfactual_value)","title":"<code>counterfactual_value</code>","text":"(<code>callable]</code>)           \u2013            <p>The value to use for unchosen actions. This should be provided as a callable function that returns a value. This will have no effect if <code>update_all_options</code> is set to False. The function takes as input the values of <code>outcome</code> and <code>chosen</code> (i.e., the two elements of the <code>outcome_chosen</code> argument). By default, this assumes that outcomes are binary and sets the value of unchosen actions to the complement of the value of chosen actions.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update(update_all_options)","title":"<code>update_all_options</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to update the value estimates for all options, regardless of whether they were chosen. Defaults to False.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice","title":"asymmetric_rescorla_wagner_update_choice  <code>module-attribute</code>","text":"<pre><code>asymmetric_rescorla_wagner_update_choice: Array = jit(asymmetric_rescorla_wagner_update_choice, static_argnums=(5, 6))\n</code></pre> <p>Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function.</p> <p>See <code>asymmetric_rescorla_wagner_update</code> for details on the learning rule.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Tuple[jax.Array, Tuple[jax.Array, jax.Array, int, jax.Array]]: - updated_value (jax.Array): The updated value estimate. - output_tuple (Tuple[jax.Array, jax.Array, int,     jax.Array]):     - value (jax.Array): The original value estimate.     - choice_p (jax.Array): The choice probabilities.     - choice (int): The chosen action.     - choice_array (jax.Array): The chosen action in one-hot         format.</p> </li> </ul>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current value estimate.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(outcome_key)","title":"<code>outcome_key</code>","text":"(<code>Tuple[ArrayLike, PRNGKey]</code>)           \u2013            <p>A tuple containing the outcome and the PRNG key.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(alpha_p)","title":"<code>alpha_p</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for positive outcomes.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(alpha_n)","title":"<code>alpha_n</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for negative outcomes.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(counterfactual_value)","title":"<code>counterfactual_value</code>","text":"(<code>callable]</code>)           \u2013            <p>The value to use for unchosen actions. This should be provided as a callable function that returns a value. This will have no effect if <code>update_all_options</code> is set to False. The function takes as input the values of <code>outcome</code> and <code>chosen</code> (i.e., the two elements of the <code>outcome_chosen</code> argument). By default, this assumes that outcomes are binary and sets the value of unchosen actions to the complement of the value of chosen actions.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice(update_all_options)","title":"<code>update_all_options</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to update the value estimates for all options, regardless of whether they were</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky","title":"asymmetric_rescorla_wagner_update_choice_sticky  <code>module-attribute</code>","text":"<pre><code>asymmetric_rescorla_wagner_update_choice_sticky: Array = jit(asymmetric_rescorla_wagner_update_choice_sticky, static_argnums=(6, 7))\n</code></pre> <p>Updates the value estimate using the asymmetric Rescorla-Wagner algorithm, and chooses an option based on the softmax function.</p> <p>Incorporates additional choice stickiness parameter, such that the probability of choosing the same option as the previous trial is increased (or decreased if the value is negative).</p> <p>See <code>asymmetric_rescorla_wagner_update</code> for details on the learning rule.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Tuple[jax.Array, Tuple[jax.Array, jax.Array, int, jax.Array]]: - updated_value (jax.Array): The updated value estimate. - output_tuple (Tuple[jax.Array, jax.Array, int,     jax.Array]):     - value (jax.Array): The original value estimate.     - choice_p (jax.Array): The choice probabilities.     - choice (int): The chosen action.     - choice_array (jax.Array): The chosen action in one-hot         format.</p> </li> </ul>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(value_choice)","title":"<code>value_choice</code>","text":"(<code>Tuple[ArrayLike, ArrayLike]</code>)           \u2013            <p>A tuple containing the current value estimate and the previous choice. The previous choice should be a one-hot encoded array of shape (n_actions,) where 1 indicates the chosen action and 0 indicates the unchosen actions.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(outcome_key)","title":"<code>outcome_key</code>","text":"(<code>Tuple[ArrayLike, PRNGKey]</code>)           \u2013            <p>A tuple containing the outcome and the PRNG key.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(alpha_p)","title":"<code>alpha_p</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for positive outcomes.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(alpha_n)","title":"<code>alpha_n</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate for negative outcomes.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(stickiness)","title":"<code>stickiness</code>","text":"(<code>float</code>)           \u2013            <p>The stickiness parameter for softmax function.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(counterfactual_value)","title":"<code>counterfactual_value</code>","text":"(<code>callable]</code>)           \u2013            <p>The value to use for unchosen actions. This should be provided as a callable function that returns a value. This will have no effect if <code>update_all_options</code> is set to False. The function takes as input the values of <code>outcome</code> and <code>chosen</code> (i.e., the two elements of the <code>outcome_chosen</code> argument). Defaults to <code>lambda x, y: (1 - x) * (1 - y)</code>, which assumes outcomes are binary (0 or 1), and sets the value of unchosen actions to complement the value of chosen actions (i.e., a chosen value of 1 will set the unchosen value to 0 and vice versa).</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_rescorla_wagner_update_choice_sticky(update_all_options)","title":"<code>update_all_options</code>","text":"(<code>bool</code>)           \u2013            <p>Whether to update the value estimates for all options, regardless of whether they were</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice","title":"asymmetric_volatile_dynamic_rescorla_wagner_update_choice  <code>module-attribute</code>","text":"<pre><code>asymmetric_volatile_dynamic_rescorla_wagner_update_choice: Array = jit(asymmetric_volatile_dynamic_rescorla_wagner_update_choice, static_argnums=(7,))\n</code></pre> <p>Updates the value estimate using a variant of the Rescorla-Wagner learning rule that adjusts learning rate based on volatility and prediction error sign, and chooses an option based on the softmax function.</p> <p>Note that learning rates for this function are transformed using a sigmoid function to ensure they are between 0 and 1. The raw parameter values supplied to the function must therefore be unbounded.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Tuple[jax.Array, Tuple[jax.typing.ArrayLike, jax.Array, int, jax.Array]]: - updated_value (jax.Array): The updated value estimate. - output_tuple (Tuple[jax.Array, jax.Array, int,     jax.Array]):     - value (jax.Array): The original value estimate.     - choice_p (jax.Array): The choice probabilities.     - choice (int): The chosen action.     - choice_array (jax.Array): The chosen action in one-hot         format.</p> </li> </ul>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current value estimate.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_base)","title":"<code>alpha_base</code>","text":"(<code>float</code>)           \u2013            <p>The base learning rate.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_volatility)","title":"<code>alpha_volatility</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for volatile outcomes.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_pos_neg)","title":"<code>alpha_pos_neg</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for positive and negative prediction errors.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(alpha_interaction)","title":"<code>alpha_interaction</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for the interaction between volatility and prediction error sign.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_dynamic_rescorla_wagner_update_choice(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice","title":"asymmetric_volatile_rescorla_wagner_single_value_update_choice  <code>module-attribute</code>","text":"<pre><code>asymmetric_volatile_rescorla_wagner_single_value_update_choice: Array = jit(asymmetric_volatile_rescorla_wagner_single_value_update_choice)\n</code></pre> <p>Updates the value estimate using the asymmetric volatile dynamic Rescorla-Wagner algorithm, and chooses an option based on the softmax function.</p> <p>This version of the function is designed for cases where the a single value is being learnt, and this value is used to determine which of two options to choose. In practice, the value of option 1 is learnt, and the value of option 2 is set to 1 - value. This is appropriate for cases where the value of one option is the complement of the other.</p> <p>Note that learning rates for this function are transformed using a sigmoid function to ensure they are between 0 and 1. The raw parameter values supplied to the function must therefore be unbounded.</p> <p>Parameters:</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current value estimate.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_base)","title":"<code>alpha_base</code>","text":"(<code>float</code>)           \u2013            <p>The base learning rate.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_volatility)","title":"<code>alpha_volatility</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for volatile outcomes.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_pos_neg)","title":"<code>alpha_pos_neg</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for positive and negative prediction errors.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice(alpha_interaction)","title":"<code>alpha_interaction</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for the interaction between volatility and prediction error sign.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>)           \u2013            <p>The temperature parameter for softmax function.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_single_value_update_choice(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>The number of actions to choose from.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_update","title":"asymmetric_volatile_rescorla_wagner_update","text":"<pre><code>asymmetric_volatile_rescorla_wagner_update(value: ArrayLike, outcome_chosen_volatility: Tuple[ArrayLike, ArrayLike, ArrayLike], alpha_base: float, alpha_volatility: float, alpha_pos_neg: float, alpha_interaction: float) -&gt; Tuple[ArrayLike, Tuple[ArrayLike, ArrayLike]]\n</code></pre> <p>Updates the estimated value of a state or action using a variant of the Rescorla-Wagner learning rule that incorporates adjusting the learning rate based on both volatility and prediction error sign.</p> <p>Note that learning rates for this function are transformed using a sigmoid function to ensure they are between 0 and 1. The raw parameter values supplied to the function must therefore be unbounded.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ArrayLike, Tuple[ArrayLike, ArrayLike]]</code>           \u2013            <p>Tuple[jax.typing.ArrayLike, Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]]: - updated_value (jax.typing.ArrayLike): The updated value estimate. - output_tuple (Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]):     - value (jax.typing.ArrayLike): The original value estimate.     - prediction_error (jax.typing.ArrayLike): The prediction       error.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/rescorla_wagner.py</code> <pre><code>@jax.jit\ndef asymmetric_volatile_rescorla_wagner_update(\n    value: jax.typing.ArrayLike,\n    outcome_chosen_volatility: Tuple[\n        jax.typing.ArrayLike,\n        jax.typing.ArrayLike,\n        jax.typing.ArrayLike,\n    ],\n    alpha_base: float,\n    alpha_volatility: float,\n    alpha_pos_neg: float,\n    alpha_interaction: float,\n) -&gt; Tuple[\n    jax.typing.ArrayLike, Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]\n]:\n    \"\"\"\n    Updates the estimated value of a state or action using a variant\n    of the Rescorla-Wagner learning rule that incorporates adjusting\n    the learning rate based on both volatility and prediction error sign.\n\n    Note that learning rates for this function are transformed using a\n    sigmoid function to ensure they are between 0 and 1. The raw\n    parameter values supplied to the function must therefore be\n    unbounded.\n\n    Args:\n        value (jax.typing.ArrayLike): The current estimated value of a\n            state or action.\n        outcome_chosen_volatility (Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike,\n            jax.typing.ArrayLike]): A tuple containing the outcome, the chosen\n            action, and the volatility indicator. The outcome is a float or an\n            array (e.g., for a single outcome or multiple outcomes). The chosen\n            action is a one-hot encoded array of shape (n_actions,) where 1\n            indicates the chosen action and 0 indicates the unchosen actions.\n            The volatility indicator is a binary value that indicates whether\n            the outcome is volatile (1) or stable (0).\n        alpha_base (float): The base learning rate.\n        alpha_volatility (float): The learning rate adjustment for volatile\n            outcomes.\n        alpha_pos_neg (float): The learning rate adjustment for positive and\n            negative prediction errors.\n        alpha_interaction (float): The learning rate adjustment for the\n            interaction between volatility and prediction error sign.\n\n    Returns:\n        Tuple[jax.typing.ArrayLike, Tuple[jax.typing.ArrayLike,\n            jax.typing.ArrayLike]]:\n            - updated_value (jax.typing.ArrayLike): The updated value estimate.\n            - output_tuple (Tuple[jax.typing.ArrayLike, jax.typing.ArrayLike]):\n                - value (jax.typing.ArrayLike): The original value estimate.\n                - prediction_error (jax.typing.ArrayLike): The prediction\n                  error.\n    \"\"\"\n\n    # Unpack the outcome and the chosen action\n    outcome, chosen, volatility_indicator = outcome_chosen_volatility\n\n    # Calculate the prediction error\n    prediction_error = outcome - value\n\n    # Set prediction error to 0 for unchosen actions\n    prediction_error = prediction_error * chosen\n\n    # Determine whether the error is positive (1) or negative (-1)\n    PE_sign = jnp.sign(prediction_error)\n\n    # Compute interaction term (volatility_indicator * error_sign)\n    interaction_term = volatility_indicator * PE_sign\n\n    # Compute the dynamic learning rate using base, volatility, and interaction terms\n    # Remember we can't use if else statements here because JAX doesn't tolerate them\n    # Use adjusted learning rates for positive/negative prediction errors\n    alpha_t = jax.nn.sigmoid(\n        alpha_base\n        + alpha_volatility * volatility_indicator\n        + alpha_pos_neg * PE_sign\n        + alpha_interaction * interaction_term\n    )\n\n    # Update the value\n    updated_value = value + alpha_t * prediction_error\n\n    return updated_value, (value, prediction_error)\n</code></pre>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_update(value)","title":"<code>value</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The current estimated value of a state or action.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_update(alpha_base)","title":"<code>alpha_base</code>","text":"(<code>float</code>)           \u2013            <p>The base learning rate.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_update(alpha_volatility)","title":"<code>alpha_volatility</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for volatile outcomes.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_update(alpha_pos_neg)","title":"<code>alpha_pos_neg</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for positive and negative prediction errors.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.asymmetric_volatile_rescorla_wagner_update(alpha_interaction)","title":"<code>alpha_interaction</code>","text":"(<code>float</code>)           \u2013            <p>The learning rate adjustment for the interaction between volatility and prediction error sign.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.complement_counterfactual","title":"complement_counterfactual","text":"<pre><code>complement_counterfactual(reward, chosen)\n</code></pre> <p>Counterfactual function that sets the value of unchosen actions to the complement of the value of chosen actions.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li>           \u2013            <p>jax.typing.ArrayLike: The counterfactual value.</p> </li> </ul> Source code in <code>behavioural_modelling/learning/rescorla_wagner.py</code> <pre><code>def complement_counterfactual(reward, chosen):\n    \"\"\"\n    Counterfactual function that sets the value of unchosen actions to the\n    complement of the value of chosen actions.\n\n    Args:\n        reward (jax.typing.ArrayLike): The reward received.\n        chosen (jax.typing.ArrayLike): A binary array indicating which\n            action(s) were chosen.\n\n    Returns:\n        jax.typing.ArrayLike: The counterfactual value.\n    \"\"\"\n    return jnp.where(\n        chosen == 1, \n        0.0, \n        1.0 - jnp.sum(reward * jnp.asarray(chosen == 1, dtype=reward.dtype))\n    )\n</code></pre>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.complement_counterfactual(reward)","title":"<code>reward</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>The reward received.</p>"},{"location":"reference/learning/rescorla_wagner/#behavioural_modelling.learning.rescorla_wagner.complement_counterfactual(chosen)","title":"<code>chosen</code>","text":"(<code>ArrayLike</code>)           \u2013            <p>A binary array indicating which action(s) were chosen.</p>"},{"location":"reference/planning/dynamic_programming/","title":"Dynamic Programming","text":""},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming","title":"dynamic_programming","text":"<p>Functions:</p> <ul> <li> <code>get_state_action_values</code>             \u2013              <p>Calculates the value of each action for a given state. Used within the main</p> </li> <li> <code>state_value_iterator</code>             \u2013              <p>Core value iteration function - calculates value function for the MDP and</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>solve_value_iteration</code>               (<code>Tuple[ndarray, ndarray]</code>)           \u2013            <p>Solves an MDP using value iteration given a reward function.</p> </li> </ul>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration","title":"solve_value_iteration  <code>module-attribute</code>","text":"<pre><code>solve_value_iteration: Tuple[ndarray, ndarray] = jit(solve_value_iteration, static_argnums=(0, 1))\n</code></pre> <p>Solves an MDP using value iteration given a reward function.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ndarray, ndarray]</code>           \u2013            <p>Tuple[jnp.ndarray, jnp.ndarray]: Final value function and action values (Q-values)</p> </li> </ul>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration(n_states)","title":"<code>n_states</code>","text":"(<code>int</code>)           \u2013            <p>Number of states</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>Number of actions</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration(reward_function)","title":"<code>reward_function</code>","text":"(<code>ndarray</code>)           \u2013            <p>Reward function (i.e., reward at each state)</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration(max_iter)","title":"<code>max_iter</code>","text":"(<code>int</code>)           \u2013            <p>Maximum number of iterations</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration(discount)","title":"<code>discount</code>","text":"(<code>float</code>)           \u2013            <p>Discount factor</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration(sas)","title":"<code>sas</code>","text":"(<code>ndarray</code>)           \u2013            <p>State-action-state transition probabilities</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.solve_value_iteration(tol)","title":"<code>tol</code>","text":"(<code>float</code>)           \u2013            <p>Tolerance for convergence</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.get_state_action_values","title":"get_state_action_values","text":"<pre><code>get_state_action_values(s: int, n_actions: int, sas: ndarray, reward: ndarray, discount: float, values: ndarray) -&gt; ndarray\n</code></pre> <p>Calculates the value of each action for a given state. Used within the main value iteration loop.</p> <p>Reward is typically conceived of as resulting from taking action A in state S. Here, we for the sake of simplicity, we assume that the reward results from visiting state S' - that is, taking action A in state S isn't rewarding in itself, but the reward received is dependent on the reward present in state S'.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Estimated value of each state</p> </li> </ul> Source code in <code>behavioural_modelling/planning/dynamic_programming.py</code> <pre><code>def get_state_action_values(\n    s: int,\n    n_actions: int,\n    sas: jnp.ndarray,\n    reward: jnp.ndarray,\n    discount: float,\n    values: jnp.ndarray,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Calculates the value of each action for a given state. Used within the main\n    value iteration loop.\n\n    Reward is typically conceived of as resulting from taking action A in state\n    S. Here, we for the sake of simplicity, we assume that the reward results\n    from visiting state S' - that is, taking action A in state S isn't\n    rewarding in itself, but the reward received is dependent on the reward\n    present in state S'.\n\n    Args:\n        s (int): State ID\n        n_actions (int): Number of possible actions\n        sas (np.ndarray): State, action, state transition function\n        reward (np.ndarray): Reward available at each state\n        discount (float): Discount factor\n        values (np.ndarray): Current estimate of value function\n\n\n    Returns:\n        np.ndarray: Estimated value of each state\n    \"\"\"\n\n    def action_update(s, a, sas, reward, discount, values):\n        p_sprime = sas[s, a, :]\n        return jnp.dot(p_sprime, reward + discount * values)\n\n    action_values = jax.vmap(\n        action_update, in_axes=(None, 0, None, None, None, None)\n    )(s, jnp.arange(n_actions, dtype=int), sas, reward, discount, values)\n\n    return action_values\n</code></pre>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.get_state_action_values(s)","title":"<code>s</code>","text":"(<code>int</code>)           \u2013            <p>State ID</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.get_state_action_values(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>Number of possible actions</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.get_state_action_values(sas)","title":"<code>sas</code>","text":"(<code>ndarray</code>)           \u2013            <p>State, action, state transition function</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.get_state_action_values(reward)","title":"<code>reward</code>","text":"(<code>ndarray</code>)           \u2013            <p>Reward available at each state</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.get_state_action_values(discount)","title":"<code>discount</code>","text":"(<code>float</code>)           \u2013            <p>Discount factor</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.get_state_action_values(values)","title":"<code>values</code>","text":"(<code>ndarray</code>)           \u2013            <p>Current estimate of value function</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.state_value_iterator","title":"state_value_iterator","text":"<pre><code>state_value_iterator(values: ndarray, reward: ndarray, discount: float, sas: ndarray, soft: bool = False) -&gt; Tuple[ndarray, float, ndarray]\n</code></pre> <p>Core value iteration function - calculates value function for the MDP and returns q-values for each action in each state.</p> <p>This function just runs one iteration of the value iteration algorithm.</p> <p>\"Soft\" value iteration can optionally be performed. This essentially involves taking the softmax of action values rather than the max, and is useful for inverse reinforcement learning (see Bloem &amp; Bambos, 2014).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Tuple[ndarray, float, ndarray]</code>           \u2013            <p>Tuple[np.ndarray, float, np.ndarray]: Returns new estimate of the value function, new delta, and new q_values</p> </li> </ul> Source code in <code>behavioural_modelling/planning/dynamic_programming.py</code> <pre><code>def state_value_iterator(\n    values: jnp.ndarray,\n    reward: jnp.ndarray,\n    discount: float,\n    sas: jnp.ndarray,\n    soft: bool = False,\n) -&gt; Tuple[jnp.ndarray, float, jnp.ndarray]:\n    \"\"\"\n    Core value iteration function - calculates value function for the MDP and\n    returns q-values for each action in each state.\n\n    This function just runs one iteration of the value iteration algorithm.\n\n    \"Soft\" value iteration can optionally be performed. This essentially\n    involves taking the softmax of action values rather than the max, and is\n    useful for inverse reinforcement learning (see Bloem &amp; Bambos, 2014).\n\n    Args:\n        values (np.ndarray): Current estimate of the value function\n        reward (np.ndarray): Reward at each state (i.e. features x reward\n            function)\n        discount (float): Discount factor\n        sas (np.ndarray): State, action, state transition function\n        soft (bool, optional): If True, this implements \"soft\" value iteration\n            rather than standard value iteration. Defaults to False.\n\n    Returns:\n        Tuple[np.ndarray, float, np.ndarray]: Returns new estimate of the value\n            function, new delta, and new q_values\n    \"\"\"\n    n_states, n_actions = sas.shape[:2]\n    q_values = jnp.zeros((n_states, n_actions))\n\n    def scan_fn(values_delta, s):\n\n        values, delta = values_delta\n\n        v = values[s]  # Current value estimate for state `s`\n        action_values = get_state_action_values(\n            s, n_actions, sas, reward, discount, values\n        )\n\n        if not soft:\n            new_value = jnp.max(action_values)\n        else:\n            new_value = jnp.log(jnp.sum(jnp.exp(action_values)) + 1e-200)\n\n        # Update Q-values for state `s`\n        q_values_s = action_values\n\n        # Update delta\n        delta = jnp.abs(new_value - v)\n\n        # Update value for state `s`\n        values = values.at[s].set(new_value)\n\n        return (values, delta), q_values_s\n\n    # Perform the sequential scan\n    (new_values, final_delta), all_q_values = jax.lax.scan(\n        scan_fn, (values, 0), jnp.arange(n_states)\n    )\n\n    # Combine all Q-values into a single array\n    q_values = q_values.at[:, :].set(all_q_values)\n\n    return new_values, final_delta, q_values\n</code></pre>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.state_value_iterator(values)","title":"<code>values</code>","text":"(<code>ndarray</code>)           \u2013            <p>Current estimate of the value function</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.state_value_iterator(reward)","title":"<code>reward</code>","text":"(<code>ndarray</code>)           \u2013            <p>Reward at each state (i.e. features x reward function)</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.state_value_iterator(discount)","title":"<code>discount</code>","text":"(<code>float</code>)           \u2013            <p>Discount factor</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.state_value_iterator(sas)","title":"<code>sas</code>","text":"(<code>ndarray</code>)           \u2013            <p>State, action, state transition function</p>"},{"location":"reference/planning/dynamic_programming/#behavioural_modelling.planning.dynamic_programming.state_value_iterator(soft)","title":"<code>soft</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, this implements \"soft\" value iteration rather than standard value iteration. Defaults to False.</p>"}]}